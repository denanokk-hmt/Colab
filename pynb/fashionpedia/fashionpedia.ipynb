{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"fashionpedia.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPD5HnaqobCPCYVDu/1oy53"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1HTVVoUA20gN","colab_type":"text"},"source":["# [TPU Object Detection and Segmentation Framework](https://github.com/tensorflow/tpu/tree/master/models/official/detection)"]},{"cell_type":"code","metadata":{"id":"2-hVj0_I2_KW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1598965674842,"user_tz":-540,"elapsed":809,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"07b322ec-77f5-4028-f2be-bc92f9203bf4"},"source":["%tensorflow_version 1.x"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aBjLgwD83OcA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1598965680077,"user_tz":-540,"elapsed":6036,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"f7518673-db08-4660-bceb-87a4669e5740"},"source":["import tensorflow\n","print(tensorflow.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["1.15.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6PL0olkb3ZwH","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598965680078,"user_tz":-540,"elapsed":6035,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}}},"source":["#!apt-get install -y python-tk"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"608sx1vj3cnG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":280},"executionInfo":{"status":"ok","timestamp":1598965687732,"user_tz":-540,"elapsed":13683,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"f983408f-9b7f-49d0-fec6-7b15bc2a4373"},"source":["!pip install --user Cython matplotlib opencv-python-headless pyyaml Pillow"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.21)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n","Collecting opencv-python-headless\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/2a/496e06fd289c01dc21b11970be1261c87ce1cc22d5340c14b516160822a7/opencv_python_headless-4.4.0.42-cp36-cp36m-manylinux2014_x86_64.whl (36.6MB)\n","\u001b[K     |████████████████████████████████| 36.6MB 82kB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (7.0.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.5)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n","Installing collected packages: opencv-python-headless\n","Successfully installed opencv-python-headless-4.4.0.42\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HfirMKQ03fPQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":192},"executionInfo":{"status":"ok","timestamp":1598965689933,"user_tz":-540,"elapsed":15878,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"63fad25a-d511-4673-c990-3f60e3071c0d"},"source":["!pip install --user 'git+https://github.com/cocodataset/cocoapi#egg=pycocotools&subdirectory=PythonAPI'"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pycocotools from git+https://github.com/cocodataset/cocoapi#egg=pycocotools&subdirectory=PythonAPI in /usr/local/lib/python3.6/dist-packages (2.0.1)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools) (3.2.2)\n","Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools) (49.6.0)\n","Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools) (0.29.21)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.18.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fP19pV-x3kYf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"d918b541-b3a6-469e-8bf3-108ffb25ad6e"},"source":["!git clone https://github.com/tensorflow/tpu/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'tpu'...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PepCBSBa3lt6","colab_type":"code","colab":{}},"source":["%cd tpu"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uOuLJxLa5oLn","colab_type":"code","colab":{}},"source":["%env"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4qtIOQko5wxv","colab_type":"code","colab":{}},"source":["%env PYTHONPATH=/tensorflow-1.15.2/python3.6:/env/python:/content/tpu/models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sq88KKlg71MF","colab_type":"code","colab":{}},"source":["%env PATH=/tensorflow-1.15.2/python3.6/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin:/content/tpu/models/official/detection:/content/tpu/models/official/efficientnet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"flJ2CiO42ZXf","colab_type":"text"},"source":["# [Fashionpedia: Ontology, Segmentation, and an Attribute Localization Dataset](https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/fashionpedia)"]},{"cell_type":"markdown","metadata":{"id":"zyga2TUsTkdq","colab_type":"text"},"source":["### [Fashionpedia Dataset](https://github.com/cvdfoundation/fashionpedia)"]},{"cell_type":"code","metadata":{"id":"93YhghJs2aN9","colab_type":"code","colab":{}},"source":["%cd /content/tpu/models/official/detection/projects/fashionpedia"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FV4URe54BeBF","colab_type":"code","colab":{}},"source":["%cd dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JqLQdHGp2Lj5","colab_type":"code","colab":{}},"source":["!wget https://s3.amazonaws.com/ifashionist-dataset/images/train2020.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X9nPMGTT9g-x","colab_type":"code","colab":{}},"source":["!unzip train2020.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jgKUj3WNBvfO","colab_type":"code","colab":{}},"source":["!wget https://s3.amazonaws.com/ifashionist-dataset/images/val_test2020.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"48ex4wAqBzT2","colab_type":"code","colab":{}},"source":["!unzip val_test2020.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVeoS7SbCT1Z","colab_type":"code","colab":{}},"source":["%cd ../"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x0UP1N469AxP","colab_type":"text"},"source":["## Model Inference"]},{"cell_type":"markdown","metadata":{"id":"8GnVgHQJ9NDB","colab_type":"text"},"source":["### Use checkpoint"]},{"cell_type":"code","metadata":{"id":"f28Vmh269CNY","colab_type":"code","colab":{}},"source":["# MODEL=\"retinanet\"\n","# IMAGE_SIZE=640\n","# CHECKPOINT_PATH=\"<path to the checkpoint>\"\n","# PARAMS_OVERRIDE=\"\"  # if any.\n","# LABEL_MAP_FILE=\"~/tpu/models/official/detection/datasets/coco_label_map.csv\"\n","# IMAGE_FILE_PATTERN=\"<path to the JPEG image that you want to run inference on>\"\n","# OUTPUT_HTML=\"./test.html\"\n","# python ~/tpu/models/official/detection/inference.py \\\n","#   --model=\"${MODEL?}\" \\\n","#   --image_size=${IMAGE_SIZE?} \\\n","#   --checkpoint_path=\"${CHECKPOINT_PATH?}\" \\\n","#   --label_map_file=\"${LABEL_MAP_FILE?}\" \\\n","#   --image_file_pattern=\"${IMAGE_FILE_PATTERN?}\" \\\n","#   --output_html=\"${OUTPUT_HTML?}\" \\\n","#   --max_boxes_to_draw=10 \\\n","#   --min_score_threshold=0.05\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M2sC74llT8Os","colab_type":"text"},"source":["### [Checkpoint](https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/fashionpedia)"]},{"cell_type":"code","metadata":{"id":"4Cu8cFJRCi_S","colab_type":"code","colab":{}},"source":["!wget https://storage.googleapis.com/cloud-tpu-checkpoints/detection/projects/fashionpedia/fashionpedia-spinenet-49.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YIZT0dPtCvHK","colab_type":"code","colab":{}},"source":["!tar zxvf fashionpedia-spinenet-49.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9EgWyJCTHyxn","colab_type":"code","colab":{}},"source":["!wget https://storage.googleapis.com/cloud-tpu-checkpoints/detection/projects/fashionpedia/fashionpedia-r50-fpn.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nhfIkEOFH9uO","colab_type":"code","colab":{}},"source":["!tar zxvf fashionpedia-r50-fpn.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-Q4dQWAIAtG","colab_type":"code","colab":{}},"source":["!wget https://storage.googleapis.com/cloud-tpu-checkpoints/detection/projects/fashionpedia/fashionpedia-r101-fpn.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CgIQ1Yf7N5nJ","colab_type":"code","colab":{}},"source":["!tar zxvf fashionpedia-r101-fpn.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NRPqKPqUE4hd","colab_type":"code","colab":{}},"source":["%%writefile inference.py\n","# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# ==============================================================================\n","# pylint: disable=line-too-long\n","r\"\"\"A stand-alone binary to run model inference and visualize results.\n","\n","It currently only supports model of type `retinanet` and `mask_rcnn`. It only\n","supports running on CPU/GPU with batch size 1.\n","\"\"\"\n","# pylint: enable=line-too-long\n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import sys\n","sys.path.insert(0, \"/content/tpu/models/official/detection\")\n","sys.path.insert(1, \"/content/tpu/models/official/efficientnet\")\n","\n","import base64\n","import csv\n","import io\n","\n","from absl import flags\n","from absl import logging\n","\n","import numpy as np\n","from PIL import Image\n","from pycocotools import mask as mask_api\n","import tensorflow.compat.v1 as tf\n","\n","from dataloader import mode_keys\n","from projects.fashionpedia.configs import factory as config_factory\n","from projects.fashionpedia.modeling import factory as model_factory\n","from utils import box_utils\n","from utils import input_utils\n","from utils import mask_utils\n","from utils.object_detection import visualization_utils\n","from hyperparameters import params_dict\n","\n","\n","FLAGS = flags.FLAGS\n","\n","flags.DEFINE_string(\n","    'model', 'attribute_mask_rcnn', 'Support `attribute_mask_rcnn`.')\n","flags.DEFINE_integer('image_size', 640, 'The image size.')\n","flags.DEFINE_string(\n","    'checkpoint_path', '', 'The path to the checkpoint file.')\n","flags.DEFINE_string(\n","    'config_file', '', 'The config file template.')\n","flags.DEFINE_string(\n","    'params_override', '', 'The YAML file/string that specifies the parameters '\n","    'override in addition to the `config_file`.')\n","flags.DEFINE_string(\n","    'label_map_file', '',\n","    'The label map file. See --label_map_format for the definition.')\n","flags.DEFINE_string(\n","    'label_map_format', 'csv',\n","    'The format of the label map file. Currently only support `csv` where the '\n","    'format of each row is: `id:name`.')\n","flags.DEFINE_string(\n","    'image_file_pattern', '',\n","    'The glob that specifies the image file pattern.')\n","flags.DEFINE_string(\n","    'output_html', '/tmp/test.html',\n","    'The output HTML file that includes images with rendered detections.')\n","flags.DEFINE_string(\n","    'output_file', '/tmp/res.npy',\n","    'The output npy file that includes model output.')\n","flags.DEFINE_integer(\n","    'max_boxes_to_draw', 10, 'The maximum number of boxes to draw.')\n","flags.DEFINE_float(\n","    'min_score_threshold', 0.05,\n","    'The minimum score thresholds in order to draw boxes.')\n","\n","\n","def main(unused_argv):\n","  del unused_argv\n","  # Load the label map.\n","  print(' - Loading the label map...')\n","  label_map_dict = {}\n","  if FLAGS.label_map_format == 'csv':\n","    with tf.gfile.Open(FLAGS.label_map_file, 'r') as csv_file:\n","      reader = csv.reader(csv_file, delimiter=':')\n","      for row in reader:\n","        if len(row) != 2:\n","          raise ValueError('Each row of the csv label map file must be in '\n","                           '`id:name` format.')\n","        id_index = int(row[0])\n","        name = row[1]\n","        label_map_dict[id_index] = {\n","            'id': id_index,\n","            'name': name,\n","        }\n","  else:\n","    raise ValueError(\n","        'Unsupported label map format: {}.'.format(FLAGS.label_mape_format))\n","\n","  params = config_factory.config_generator(FLAGS.model)\n","  if FLAGS.config_file:\n","    params = params_dict.override_params_dict(\n","        params, FLAGS.config_file, is_strict=True)\n","  params = params_dict.override_params_dict(\n","      params, FLAGS.params_override, is_strict=True)\n","  params.override({\n","      'architecture': {\n","          'use_bfloat16': False,  # The inference runs on CPU/GPU.\n","      },\n","  }, is_strict=True)\n","  params.validate()\n","  params.lock()\n","\n","  model = model_factory.model_generator(params)\n","\n","  with tf.Graph().as_default():\n","    image_input = tf.placeholder(shape=(), dtype=tf.string)\n","    image = tf.io.decode_image(image_input, channels=3)\n","    image.set_shape([None, None, 3])\n","\n","    image = input_utils.normalize_image(image)\n","    image_size = [FLAGS.image_size, FLAGS.image_size]\n","    image, image_info = input_utils.resize_and_crop_image(\n","        image,\n","        image_size,\n","        image_size,\n","        aug_scale_min=1.0,\n","        aug_scale_max=1.0)\n","    image.set_shape([image_size[0], image_size[1], 3])\n","\n","    # batching.\n","    images = tf.reshape(image, [1, image_size[0], image_size[1], 3])\n","    images_info = tf.expand_dims(image_info, axis=0)\n","\n","    # model inference\n","    outputs = model.build_outputs(\n","        images, {'image_info': images_info}, mode=mode_keys.PREDICT)\n","\n","    outputs['detection_boxes'] = (\n","        outputs['detection_boxes'] / tf.tile(images_info[:, 2:3, :], [1, 1, 2]))\n","\n","    predictions = outputs\n","\n","    # Create a saver in order to load the pre-trained checkpoint.\n","    saver = tf.train.Saver()\n","\n","    image_with_detections_list = []\n","    with tf.Session() as sess:\n","      print(' - Loading the checkpoint...')\n","      saver.restore(sess, FLAGS.checkpoint_path)\n","\n","      res = []\n","      image_files = tf.gfile.Glob(FLAGS.image_file_pattern)\n","      for i, image_file in enumerate(image_files):\n","        print(' - Processing image %d...' % i)\n","\n","        with tf.gfile.GFile(image_file, 'rb') as f:\n","          image_bytes = f.read()\n","\n","        image = Image.open(image_file)\n","        image = image.convert('RGB')  # needed for images with 4 channels.\n","        width, height = image.size\n","        np_image = (np.array(image.getdata())\n","                    .reshape(height, width, 3).astype(np.uint8))\n","\n","        predictions_np = sess.run(\n","            predictions, feed_dict={image_input: image_bytes})\n","\n","        num_detections = int(predictions_np['num_detections'][0])\n","        np_boxes = predictions_np['detection_boxes'][0, :num_detections]\n","        np_scores = predictions_np['detection_scores'][0, :num_detections]\n","        np_classes = predictions_np['detection_classes'][0, :num_detections]\n","        np_classes = np_classes.astype(np.int32)\n","        np_attributes = predictions_np['detection_attributes'][\n","            0, :num_detections, :]\n","        np_masks = None\n","        if 'detection_masks' in predictions_np:\n","          instance_masks = predictions_np['detection_masks'][0, :num_detections]\n","          np_masks = mask_utils.paste_instance_masks(\n","              instance_masks, box_utils.yxyx_to_xywh(np_boxes), height, width)\n","          encoded_masks = [\n","              mask_api.encode(np.asfortranarray(np_mask))\n","              for np_mask in list(np_masks)]\n","\n","        res.append({\n","            'image_file': image_file,\n","            'boxes': np_boxes,\n","            'classes': np_classes,\n","            'scores': np_scores,\n","            'attributes': np_attributes,\n","            'masks': encoded_masks,\n","        })\n","\n","        image_with_detections = (\n","            visualization_utils.visualize_boxes_and_labels_on_image_array(\n","                np_image,\n","                np_boxes,\n","                np_classes,\n","                np_scores,\n","                label_map_dict,\n","                instance_masks=np_masks,\n","                use_normalized_coordinates=False,\n","                max_boxes_to_draw=FLAGS.max_boxes_to_draw,\n","                min_score_thresh=FLAGS.min_score_threshold))\n","        image_with_detections_list.append(image_with_detections)\n","\n","  print(' - Saving the outputs...')\n","  formatted_image_with_detections_list = [\n","      Image.fromarray(image.astype(np.uint8))\n","      for image in image_with_detections_list]\n","  html_str = '<html>'\n","  image_strs = []\n","  for formatted_image in formatted_image_with_detections_list:\n","    with io.BytesIO() as stream:\n","      formatted_image.save(stream, format='JPEG')\n","      data_uri = base64.b64encode(stream.getvalue()).decode('utf-8')\n","    image_strs.append(\n","        '<img src=\"data:image/jpeg;base64,{}\", height=800>'\n","        .format(data_uri))\n","  images_str = ' '.join(image_strs)\n","  html_str += images_str\n","  html_str += '</html>'\n","  with tf.gfile.GFile(FLAGS.output_html, 'w') as f:\n","    f.write(html_str)\n","  np.save(FLAGS.output_file, res)\n","\n","\n","if __name__ == '__main__':\n","  flags.mark_flag_as_required('model')\n","  flags.mark_flag_as_required('checkpoint_path')\n","  flags.mark_flag_as_required('label_map_file')\n","  flags.mark_flag_as_required('image_file_pattern')\n","  flags.mark_flag_as_required('output_html')\n","  logging.set_verbosity(logging.INFO)\n","  tf.app.run(main)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iAh1YQtyKoZh","colab_type":"code","colab":{}},"source":["!git checkout d35e485"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_oYl4vemCYB4","colab_type":"code","colab":{}},"source":["%%bash\n","MODEL=\"attribute_mask_rcnn\"\n","IMAGE_SIZE=640\n","CHECKPOINT_PATH=\"./fashionpedia-r50-fpn/model.ckpt\"\n","#CHECKPOINT_PATH=\"./fashionpedia-r101-fpn/model.ckpt\"\n","#CHECKPOINT_PATH=\"./fashionpedia-spinenet-49/model.ckpt\"\n","PARAMS_OVERRIDE=\"\"  # if any.\n","LABEL_MAP_FILE=\"./dataset/fashionpedia_label_map.csv\"\n","#IMAGE_FILE_PATTERN=\"./dataset/test/ab23d0c1ccdadcac2da79af78298ea8d.jpg\"\n","IMAGE_FILE_PATTERN=\"./dataset/train/4789aab7dffdf2ebe4ea809275df5e26.jpg\"\n","OUTPUT_HTML=\"./test.html\"\n","python ./inference.py \\\n","  --model=\"${MODEL?}\" \\\n","  --image_size=${IMAGE_SIZE?} \\\n","  --checkpoint_path=\"${CHECKPOINT_PATH?}\" \\\n","  --label_map_file=\"${LABEL_MAP_FILE?}\" \\\n","  --image_file_pattern=\"${IMAGE_FILE_PATTERN?}\" \\\n","  --output_html=\"${OUTPUT_HTML?}\" \\\n","  --max_boxes_to_draw=10 \\\n","  --min_score_threshold=0.05"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AX9ddOYVQAkI","colab_type":"code","colab":{}},"source":["from IPython.display import Image,display_jpeg\n","display_jpeg(Image('./dataset/train/4789aab7dffdf2ebe4ea809275df5e26.jpg'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4o-uQQILuS_","colab_type":"code","colab":{}},"source":["import IPython\n","IPython.display.HTML(filename='./test.html')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j36Nbm129Ch7","colab_type":"text"},"source":["### Use SavedModel"]},{"cell_type":"code","metadata":{"id":"46UJALWg9DXY","colab_type":"code","colab":{}},"source":["# SAVED_MODEL_DIR=\"<path to the SavedModel>\"\n","# LABEL_MAP_FILE=\"~/tpu/models/official/detection/datasets/coco_label_map.csv\"\n","# IMAGE_FILE_PATTERN=\"<path to the JPEG image that you want to run inference on>\"\n","# OUTPUT_HTML=\"./test.html\"\n","# python ~/tpu/models/detection/inference_saved_model \\\n","#   --saved_model_dir=\"${SAVED_MODEL_DIR?}\" \\\n","#   --label_map_file=\"${LABEL_MAP_FILE?}\" \\\n","#   --image_file_pattern=\"${IMAGE_FILE_PATTERN?}\" \\\n","#   --output_html=\"${OUTPUT_HTML?}\" \\\n","#   --max_boxes_to_draw=10 \\\n","#   --min_score_threshold=0.05\n"],"execution_count":null,"outputs":[]}]}