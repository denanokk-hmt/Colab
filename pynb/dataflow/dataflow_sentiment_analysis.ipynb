{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataflow_sentiment_analysis.ipynb","provenance":[{"file_id":"1GKb0b_G-EUGKinJqVR-q72y09YVqtG55","timestamp":1643619385164}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# [Cloud Dataflow](https://cloud.google.com/dataflow/docs/quickstarts/quickstart-python?hl=ja)"],"metadata":{"id":"D4jWuVzPQsWM"}},{"cell_type":"markdown","source":["## タイムゾーンの変更"],"metadata":{"id":"JXjs1gXicveN"}},{"cell_type":"code","source":["!rm /etc/localtime\n","!ln -s /usr/share/zoneinfo/Asia/Tokyo /etc/localtime\n","!date"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SrTXnldlcxom","executionInfo":{"status":"ok","timestamp":1648622814052,"user_tz":-540,"elapsed":612,"user":{"displayName":"Support Product","userId":"10938393275403308243"}},"outputId":"8743594a-53d7-42c5-dffc-d1b325393c1b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Mar 30 15:46:53 JST 2022\n"]}]},{"cell_type":"markdown","source":["## Apache Beam SDK を入手する"],"metadata":{"id":"yJmMNUs1YGO6"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wm15eEXqPSQY","executionInfo":{"status":"ok","timestamp":1648622821325,"user_tz":-540,"elapsed":7275,"user":{"displayName":"Support Product","userId":"10938393275403308243"}},"outputId":"2d3bf664-2afb-4187-a7fa-b62c1d4599e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: apache-beam[gcp] in /usr/local/lib/python3.7/dist-packages (2.37.0)\n","Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (1.3.0)\n","Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (1.20.3)\n","Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (3.10.0.2)\n","Requirement already satisfied: pyarrow<7.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (6.0.1)\n","Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (2.7.0)\n","Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (4.1.3)\n","Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (3.19.4)\n","Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (3.12.3)\n","Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (1.4.10)\n","Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (1.45.0)\n","Requirement already satisfied: cloudpickle<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (2.0.0)\n","Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (2.8.2)\n","Requirement already satisfied: orjson<4.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (3.6.7)\n","Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (0.3.1.1)\n","Requirement already satisfied: numpy<1.22.0,>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (1.21.5)\n","Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (2.27.1)\n","Requirement already satisfied: httplib2<0.20.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (0.17.4)\n","Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (1.7)\n","Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (2022.1)\n","Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (1.35.0)\n","Requirement already satisfied: google-cloud-core<2,>=0.28.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (1.7.2)\n","Requirement already satisfied: google-cloud-bigquery<3,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (1.21.0)\n","Requirement already satisfied: google-cloud-recommendations-ai<=0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (0.2.0)\n","Requirement already satisfied: grpcio-gcp<1,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (0.2.2)\n","Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (1.7.0)\n","Requirement already satisfied: google-cloud-spanner<2,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (1.19.1)\n","Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (0.5.31)\n","Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (3.6.2)\n","Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (1.16.1)\n","Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (1.4.1)\n","Requirement already satisfied: google-cloud-vision<2,>=0.38.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (1.0.0)\n","Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (2.11.0)\n","Requirement already satisfied: google-cloud-bigquery-storage>=2.6.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (2.13.0)\n","Requirement already satisfied: cachetools<5,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (4.2.4)\n","Requirement already satisfied: google-cloud-language<2,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (1.3.0)\n","Requirement already satisfied: google-cloud-datastore<2,>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]) (1.8.0)\n","Requirement already satisfied: fasteners>=0.14 in /usr/local/lib/python3.7/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (0.17.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (1.15.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]) (0.2.8)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]) (57.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]) (4.8)\n","Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]) (0.4.1)\n","Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]) (1.31.5)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]) (21.3)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]) (1.56.0)\n","Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]) (0.12.3)\n","Requirement already satisfied: grpcio-status>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]) (1.45.0)\n","Requirement already satisfied: overrides<7.0.0,>=6.0.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]) (6.1.0)\n","Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (0.6.2)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]) (0.4.8)\n","Requirement already satisfied: typing-utils>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from overrides<7.0.0,>=6.0.1->google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]) (0.1.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]) (3.0.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (2021.10.8)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (1.24.3)\n"]}],"source":["!pip install apache-beam[gcp]"]},{"cell_type":"markdown","source":["## Google 認証"],"metadata":{"id":"gZS4z3SEWvr-"}},{"cell_type":"code","source":["from google.colab import auth\n","auth.authenticate_user()"],"metadata":{"id":"tc3KHum6RkP5","executionInfo":{"status":"ok","timestamp":1648622837365,"user_tz":-540,"elapsed":16048,"user":{"displayName":"Support Product","userId":"10938393275403308243"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## 資材準備"],"metadata":{"id":"LEzPpKlAYQfi"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"ets24nJQZoIk"}},{"cell_type":"code","source":["!touch requirements.txt"],"metadata":{"id":"FHajCeS9ZE5R","executionInfo":{"status":"ok","timestamp":1648622837937,"user_tz":-540,"elapsed":575,"user":{"displayName":"Support Product","userId":"10938393275403308243"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["%%writefile setup.py\n","#\n","# Licensed to the Apache Software Foundation (ASF) under one or more\n","# contributor license agreements.  See the NOTICE file distributed with\n","# this work for additional information regarding copyright ownership.\n","# The ASF licenses this file to You under the Apache License, Version 2.0\n","# (the \"License\"); you may not use this file except in compliance with\n","# the License.  You may obtain a copy of the License at\n","#\n","#    http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","#\n","\n","\"\"\"Setup.py module for the workflow's worker utilities.\n","\n","All the workflow related code is gathered in a package that will be built as a\n","source distribution, staged in the staging area for the workflow being run and\n","then installed in the workers when they start running.\n","\n","This behavior is triggered by specifying the --setup_file command line option\n","when running the workflow for remote execution.\n","\"\"\"\n","\n","# pytype: skip-file\n","\n","from __future__ import absolute_import\n","from __future__ import print_function\n","\n","import subprocess\n","import setuptools\n","from distutils.command.build import build as _build  # type: ignore\n","\n","# This class handles the pip install mechanism.\n","class build(_build):  # pylint: disable=invalid-name\n","  \"\"\"A build command class that will be invoked during package install.\n","\n","  The package built using the current setup.py will be staged and later\n","  installed in the worker using `pip install package'. This class will be\n","  instantiated during install for this specific scenario and will trigger\n","  running the custom commands specified.\n","  \"\"\"\n","  sub_commands = _build.sub_commands + [('CustomCommands', None)]\n","\n","\n","# Some custom command to run during setup. The command is not essential for this\n","# workflow. It is used here as an example. Each command will spawn a child\n","# process. Typically, these commands will include steps to install non-Python\n","# packages. For instance, to install a C++-based library libjpeg62 the following\n","# two commands will have to be added:\n","#\n","#     ['apt-get', 'update'],\n","#     ['apt-get', '--assume-yes', 'install', 'libjpeg62'],\n","#\n","# First, note that there is no need to use the sudo command because the setup\n","# script runs with appropriate access.\n","# Second, if apt-get tool is used then the first command needs to be 'apt-get\n","# update' so the tool refreshes itself and initializes links to download\n","# repositories.  Without this initial step the other apt-get install commands\n","# will fail with package not found errors. Note also --assume-yes option which\n","# shortcuts the interactive confirmation.\n","#\n","# Note that in this example custom commands will run after installing required\n","# packages. If you have a PyPI package that depends on one of the custom\n","# commands, move installation of the dependent package to the list of custom\n","# commands, e.g.:\n","#\n","#     ['pip', 'install', 'my_package'],\n","#\n","# TODO(BEAM-3237): Output from the custom commands are missing from the logs.\n","# The output of custom commands (including failures) will be logged in the\n","# worker-startup log.\n","CUSTOM_COMMANDS = [['apt-get', 'update'],\n","                   ['apt-get', '--assume-yes', 'install', 'mecab'],\n","                   ['apt-get', '--assume-yes', 'install', 'libmecab-dev'],\n","                   ['apt-get', '--assume-yes', 'install', 'mecab-ipadic-utf8'],\n","                   ['pip3', 'install', 'google-cloud-language==1.3.0'],\n","                   ['pip3', 'install', 'mecab-python3'],\n","                   ['apt-get', '--assume-yes', 'install', 'git', 'make', 'curl', 'xz-utils', 'file'],\n","                   ['apt-get', '--assume-yes', 'install', 'unzip'],\n","                   ['wget', 'https://github.com/neologd/mecab-ipadic-neologd/archive/master.zip', '-O', 'mecab-ipadic-neologd.zip'],\n","                   ['unzip', '-o', 'mecab-ipadic-neologd.zip'],\n","                   ['./mecab-ipadic-neologd-master/bin/install-mecab-ipadic-neologd', '-n', '-y']]\n","\n","\n","class CustomCommands(setuptools.Command):\n","  \"\"\"A setuptools Command class able to run arbitrary commands.\"\"\"\n","  def initialize_options(self):\n","    pass\n","\n","  def finalize_options(self):\n","    pass\n","\n","  def RunCustomCommand(self, command_list):\n","    print('Running command: %s' % command_list)\n","    p = subprocess.Popen(\n","        command_list,\n","        stdin=subprocess.PIPE,\n","        stdout=subprocess.PIPE,\n","        stderr=subprocess.STDOUT)\n","    # Can use communicate(input='y\\n'.encode()) if the command run requires\n","    # some confirmation.\n","    stdout_data, _ = p.communicate()\n","    print('Command output: %s' % stdout_data)\n","    if p.returncode != 0:\n","      raise RuntimeError(\n","          'Command %s failed: exit code: %s' % (command_list, p.returncode))\n","\n","  def run(self):\n","    for command in CUSTOM_COMMANDS:\n","      self.RunCustomCommand(command)\n","\n","\n","# Configure the required packages and scripts to install.\n","# Note that the Python Dataflow containers come with numpy already installed\n","# so this dependency will not trigger anything to be installed unless a version\n","# restriction is specified.\n","REQUIRED_PACKAGES = [\n","    'mecab-python3', 'unidic-lite'\n","]\n","\n","setuptools.setup(\n","    name='mecab-neologd',\n","    version='0.0.1',\n","    author='kanou',\n","    author_email='kanou@solairo.co.jp',\n","    url='https://www.solairo.co.jp', \n","    description='MeCab NEologd set workflow package.',\n","    install_requires=REQUIRED_PACKAGES,\n","    packages=setuptools.find_packages(),\n","    cmdclass={\n","        # Command class instantiated and run during pip install scenarios.\n","        'build': build,\n","        'CustomCommands': CustomCommands,\n","    })"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5J4ptJK-ZFA2","executionInfo":{"status":"ok","timestamp":1648622837938,"user_tz":-540,"elapsed":18,"user":{"displayName":"Support Product","userId":"10938393275403308243"}},"outputId":"9db685da-dd51-49d3-b309-77c8d22a924e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing setup.py\n"]}]},{"cell_type":"markdown","source":["### whatya_v2_all"],"metadata":{"id":"oe4AISXbs5yp"}},{"cell_type":"code","source":["%%writefile dataflow_whatya_v2_all.py\n","import argparse\n","import logging\n","import sys\n","from datetime import datetime\n","from apache_beam.io.gcp import gcsio\n","import apache_beam as beam\n","from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions, SetupOptions, WorkerOptions\n","\n","class AnalyzeSentiment(beam.DoFn):\n","    def process(self, context):\n","        import time\n","        from google.cloud import language\n","        from google.cloud.language import enums\n","        from google.cloud.language import types\n","        client = language.LanguageServiceClient()\n","\n","        text = context[\"content\"]\n","        language = \"ja\"\n","        document = types.Document(\n","            content=text,\n","            type=enums.Document.Type.PLAIN_TEXT,\n","            language=language)\n","    \n","        sentiment = client.analyze_sentiment(document).document_sentiment\n","\n","        num = context[\"num\"]\n","        score = sentiment.score\n","        magnitude = sentiment.magnitude\n","\n","        time.sleep(0.1)\n","\n","        return [{'date': context[\"date\"],\n","                 'client': context[\"client\"],\n","                 'content': text,\n","                 'num': num,\n","                 'score': score,\n","                 'magnitude': magnitude\n","                }]\n","\n","def pipeline(options=None, known_args=None):\n","    \"\"\"\n","    BQにクエリを投げ、別のテーブルに出力するパイプライン\n","\n","    :param options:\n","    :param known_args:\n","    :return:\n","    \"\"\"\n","    project = options.view_as(GoogleCloudOptions).project\n","    dataset = known_args.dataset\n","    input = known_args.input\n","    output = known_args.output\n","    # client = known_args.client\n","    from_date = known_args.from_date\n","    to_date = known_args.to_date\n","\n","    query = \"\"\"\n","WITH t AS (\n","  SELECT\n","    date,\n","    client,\n","    IFNULL(quest_item_name, ctalk_quest_message) AS content,\n","    count(*) as num\n","  from {}.{}.{}\n","  where\n","    (client in (SELECT DISTINCT(client_id) FROM `bwing-230309.whatya.client_master`) AND date BETWEEN DATE('{}', \"Asia/Tokyo\") AND DATE('{}', \"Asia/Tokyo\"))\n","    AND regexp_extract(quest_item_value, '(000_op)') IS NULL\n","    AND mtype in ('customer')\n","  group by date, client, quest_item_name, ctalk_quest_message\n","  order by num desc\n",")\n","SELECT\n","  date,\n","  client,\n","  content,\n","  num\n","FROM t\n","WHERE\n","    content != ''\n","    AND regexp_extract(content, '(init|init_bot|init_op|テスト|000_op|string_string_chip)') IS NULL\n","GROUP BY date, client, content, num\n","ORDER BY num DESC\n","\"\"\".format(project, dataset, input, from_date, to_date)\n","\n","    ## 新たなパイプラインを生成\n","    q = beam.Pipeline(options=options)\n","\n","    # クエリを実行\n","    #lines = q | 'ReadFromBigQuery' >> beam.io.Read(beam.io.BigQuerySource(query=query, use_standard_sql=True))\n","    lines = q | 'ReadFromBigQuery' >> beam.io.Read(beam.io.gcp.bigquery.ReadFromBigQuery(query=query, use_standard_sql=True))\n","\n","    lines = lines | 'Analyze Sentiment' >> beam.ParDo(AnalyzeSentiment())\n","\n","    # 別のテーブルに書き込む\n","    lines | 'WriteToBQ' >> beam.io.WriteToBigQuery(project=project, dataset=dataset, table=output, \n","                                                            schema='date:DATE, client:STRING, content:STRING, num:INTEGER, score:FLOAT64, magnitude:FLOAT64',\n","                                                            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n","                                                            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)\n","                                                            # write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE)\n","    # 実行する\n","    q.run().wait_until_finish()\n","\n","def run(argv=None, save_main_session=True):\n","    \"\"\"Main entry point; defines and runs the dataflow pipeline.\"\"\"\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--input', dest='input', required=True, help='Input table')\n","    parser.add_argument('--output', dest='output', required=True, help='Output table')\n","    parser.add_argument('--project', dest='project', required=True, help='project')\n","    parser.add_argument('--region', dest='region', required=True, help='region')\n","    parser.add_argument('--dataset', dest='dataset', required=True, help='dataset')\n","    # parser.add_argument('--client', dest='client', required=True, help='client')\n","    parser.add_argument('--from_date', dest='from_date', required=True, help='From Date(YYYY-MM-DD)')\n","    parser.add_argument('--to_date', dest='to_date', required=True, help='To Date(YYYY-MM-DD)')\n","    parser.add_argument('--bucket', dest='bucket', required=True, help='bucket')\n","\n","    known_args, pipeline_args = parser.parse_known_args(argv)\n","\n","    # PipelineOptionを設定\n","    options = PipelineOptions(pipeline_args)\n","\n","    google_cloud_options = options.view_as(GoogleCloudOptions)\n","    google_cloud_options.project = known_args.project\n","    google_cloud_options.region = known_args.region\n","    google_cloud_options.staging_location = '{}/code/'.format(known_args.bucket)\n","    google_cloud_options.temp_location = '{}/temp/'.format(known_args.bucket)\n","\n","    worker_options = options.view_as(WorkerOptions)\n","    worker_options.disk_size_gb = 30\n","    worker_options.max_num_workers = 1\n","    worker_options.machine_type = 'n1-standard-2'\n","    # worker_options.use_public_ips = True\n","\n","    options.view_as(StandardOptions).runner = 'DataflowRunner'\n","    setup_option = options.view_as(SetupOptions)\n","    setup_option.requirements_file = \"./requirements.txt\"\n","    setup_option.view_as(SetupOptions).setup_file = './setup.py'\n","\n","    # BQからBQ\n","    pipeline(options, known_args)\n","\n","if __name__ == '__main__':\n","    logging.getLogger().setLevel(logging.INFO)\n","    run(sys.argv)\n"],"metadata":{"id":"eY-5MntwRI4E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648622837938,"user_tz":-540,"elapsed":16,"user":{"displayName":"Support Product","userId":"10938393275403308243"}},"outputId":"3f9a96c3-775f-4b2f-e2d0-793b539bac1b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing dataflow_whatya_v2_all.py\n"]}]},{"cell_type":"markdown","source":["### whatya_v2_wlp"],"metadata":{"id":"jq2NE-jEtNL9"}},{"cell_type":"code","source":["%%writefile dataflow_whatya_v2_wlp.py\n","import argparse\n","import logging\n","import sys\n","from datetime import datetime\n","from apache_beam.io.gcp import gcsio\n","import apache_beam as beam\n","from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions, SetupOptions, WorkerOptions\n","\n","class AnalyzeSentiment(beam.DoFn):\n","    def process(self, context):\n","        import time\n","        from google.cloud import language\n","        from google.cloud.language import enums\n","        from google.cloud.language import types\n","\n","        client = language.LanguageServiceClient()\n","\n","        text = context[\"content\"]\n","        language = \"ja\"\n","        document = types.Document(\n","            content=text,\n","            type=enums.Document.Type.PLAIN_TEXT,\n","            language=language)\n","    \n","        sentiment = client.analyze_sentiment(document).document_sentiment\n","\n","        num = context[\"num\"]\n","        score = sentiment.score\n","        magnitude = sentiment.magnitude\n","\n","        time.sleep(0.1)\n","\n","        return [{'date': context[\"date\"],\n","                 'client': context[\"client\"],\n","                 'content': text,\n","                 'num': num,\n","                 'score': score,\n","                 'magnitude': magnitude\n","                }]\n","\n","def pipeline(options=None, known_args=None):\n","    \"\"\"\n","    BQにクエリを投げ、別のテーブルに出力するパイプライン\n","\n","    :param options:\n","    :param known_args:\n","    :return:\n","    \"\"\"\n","    project = options.view_as(GoogleCloudOptions).project\n","    dataset = known_args.dataset\n","    input = known_args.input\n","    output = known_args.output\n","    # client = known_args.client\n","    from_date = known_args.from_date\n","    to_date = known_args.to_date\n","\n","    query = \"\"\"\n","WITH t AS (\n","  SELECT\n","    date,\n","    client,\n","    IFNULL(quest_item_name, ctalk_quest_message) AS content,\n","    count(*) as num\n","  from {}.{}.{}\n","  where\n","    (REGEXP_EXTRACT(client, r'wlp-[0-9]+') IS NOT NULL OR client in ('lla'))\n","    AND date BETWEEN DATE('{}', \"Asia/Tokyo\") AND DATE('{}', \"Asia/Tokyo\")\n","    AND regexp_extract(quest_item_value, '(000_op)') IS NULL\n","    AND mtype in ('customer')\n","  group by date, client, quest_item_name, ctalk_quest_message\n","  order by num desc\n",")\n","SELECT\n","  date,\n","  client,\n","  content,\n","  num\n","FROM t\n","WHERE\n","    content != ''\n","    AND regexp_extract(content, '(init|init_bot|init_op|テスト|000_op|string_string_chip)') IS NULL\n","GROUP BY date, client, content, num\n","ORDER BY num DESC\n","\"\"\".format(project, dataset, input, from_date, to_date)\n","\n","    ## 新たなパイプラインを生成\n","    q = beam.Pipeline(options=options)\n","\n","    # クエリを実行\n","    #lines = q | 'ReadFromBigQuery' >> beam.io.Read(beam.io.BigQuerySource(query=query, use_standard_sql=True))\n","    lines = q | 'ReadFromBigQuery' >> beam.io.Read(beam.io.gcp.bigquery.ReadFromBigQuery(query=query, use_standard_sql=True))\n","\n","    lines = lines | 'Analyze Sentiment' >> beam.ParDo(AnalyzeSentiment())\n","\n","    # 別のテーブルに書き込む\n","    lines | 'WriteToBQ' >> beam.io.WriteToBigQuery(project=project, dataset=dataset, table=output, \n","                                                            schema='date:DATE, client:STRING, content:STRING, num:INTEGER, score:FLOAT64, magnitude:FLOAT64',\n","                                                            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n","                                                            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)\n","                                                            # write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE)\n","    # 実行する\n","    q.run().wait_until_finish()\n","\n","def run(argv=None, save_main_session=True):\n","    \"\"\"Main entry point; defines and runs the dataflow pipeline.\"\"\"\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--input', dest='input', required=True, help='Input table')\n","    parser.add_argument('--output', dest='output', required=True, help='Output table')\n","    parser.add_argument('--project', dest='project', required=True, help='project')\n","    parser.add_argument('--region', dest='region', required=True, help='region')\n","    parser.add_argument('--dataset', dest='dataset', required=True, help='dataset')\n","    # parser.add_argument('--client', dest='client', required=True, help='client')\n","    parser.add_argument('--from_date', dest='from_date', required=True, help='From Date(YYYY-MM-DD)')\n","    parser.add_argument('--to_date', dest='to_date', required=True, help='To Date(YYYY-MM-DD)')\n","    parser.add_argument('--bucket', dest='bucket', required=True, help='bucket')\n","\n","    known_args, pipeline_args = parser.parse_known_args(argv)\n","\n","    # PipelineOptionを設定\n","    options = PipelineOptions(pipeline_args)\n","\n","    google_cloud_options = options.view_as(GoogleCloudOptions)\n","    google_cloud_options.project = known_args.project\n","    google_cloud_options.region = known_args.region\n","    google_cloud_options.staging_location = '{}/code/'.format(known_args.bucket)\n","    google_cloud_options.temp_location = '{}/temp/'.format(known_args.bucket)\n","\n","    worker_options = options.view_as(WorkerOptions)\n","    worker_options.disk_size_gb = 30\n","    worker_options.max_num_workers = 1\n","    worker_options.machine_type = 'n1-standard-2'\n","    # worker_options.use_public_ips = True\n","\n","    options.view_as(StandardOptions).runner = 'DataflowRunner'\n","    setup_option = options.view_as(SetupOptions)\n","    setup_option.requirements_file = \"./requirements.txt\"\n","    setup_option.view_as(SetupOptions).setup_file = './setup.py'\n","\n","    # BQからBQ\n","    pipeline(options, known_args)\n","\n","if __name__ == '__main__':\n","    logging.getLogger().setLevel(logging.INFO)\n","    run(sys.argv)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2wshqg_Oc95e","executionInfo":{"status":"ok","timestamp":1648622837938,"user_tz":-540,"elapsed":14,"user":{"displayName":"Support Product","userId":"10938393275403308243"}},"outputId":"c1722dd5-e802-4242-a850-0cb1e2688d74"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing dataflow_whatya_v2_wlp.py\n"]}]},{"cell_type":"markdown","source":["### whatya_v2_zozo"],"metadata":{"id":"T59aUVbhtRGF"}},{"cell_type":"code","source":["%%writefile dataflow_whatya_v2_zozo.py\n","import argparse\n","import logging\n","import sys\n","from datetime import datetime\n","from apache_beam.io.gcp import gcsio\n","import apache_beam as beam\n","from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions, SetupOptions, WorkerOptions\n","\n","class AnalyzeSentiment(beam.DoFn):\n","    def process(self, context):\n","        import time\n","        from google.cloud import language\n","        from google.cloud.language import enums\n","        from google.cloud.language import types\n","        client = language.LanguageServiceClient()\n","\n","        text = context[\"content\"]\n","        language = \"ja\"\n","        document = types.Document(\n","            content=text,\n","            type=enums.Document.Type.PLAIN_TEXT,\n","            language=language)\n","    \n","        sentiment = client.analyze_sentiment(document).document_sentiment\n","\n","        num = context[\"num\"]\n","        score = sentiment.score\n","        magnitude = sentiment.magnitude\n","\n","        time.sleep(0.1)\n","\n","        return [{'date': context[\"date\"],\n","                 'client': context[\"client\"],\n","                 'content': text,\n","                 'num': num,\n","                 'score': score,\n","                 'magnitude': magnitude\n","                }]\n","\n","def pipeline(options=None, known_args=None):\n","    \"\"\"\n","    BQにクエリを投げ、別のテーブルに出力するパイプライン\n","\n","    :param options:\n","    :param known_args:\n","    :return:\n","    \"\"\"\n","    project = options.view_as(GoogleCloudOptions).project\n","    dataset = known_args.dataset\n","    input = known_args.input\n","    output = known_args.output\n","    # client = known_args.client\n","    from_date = known_args.from_date\n","    to_date = known_args.to_date\n","\n","    query = \"\"\"\n","WITH t AS (\n","  SELECT\n","    date,\n","    client,\n","    IFNULL(quest_item_name, ctalk_quest_message) AS content,\n","    count(*) as num\n","  from {}.{}.{}\n","  where\n","    (client in ('z00010j', 'z00011d', 'z00012l', 'z00013h', 'z00014a', 'z00016q', 'z00017e') AND date BETWEEN DATE('{}', \"Asia/Tokyo\") AND DATE('{}', \"Asia/Tokyo\"))\n","    AND regexp_extract(quest_item_value, '(000_op)') IS NULL\n","    AND mtype in ('customer')\n","  group by date, client, quest_item_name, ctalk_quest_message\n","  order by num desc\n",")\n","SELECT\n","  date,\n","  client,\n","  content,\n","  num\n","FROM t\n","WHERE\n","    content != ''\n","    AND regexp_extract(content, '(init|init_bot|init_op|テスト|000_op|string_string_chip)') IS NULL\n","GROUP BY date, client, content, num\n","ORDER BY num DESC\n","\"\"\".format(project, dataset, input, from_date, to_date)\n","\n","    ## 新たなパイプラインを生成\n","    q = beam.Pipeline(options=options)\n","\n","    # クエリを実行\n","    #lines = q | 'ReadFromBigQuery' >> beam.io.Read(beam.io.BigQuerySource(query=query, use_standard_sql=True))\n","    lines = q | 'ReadFromBigQuery' >> beam.io.Read(beam.io.gcp.bigquery.ReadFromBigQuery(query=query, use_standard_sql=True))\n","\n","    lines = lines | 'Analyze Sentiment' >> beam.ParDo(AnalyzeSentiment())\n","\n","    # 別のテーブルに書き込む\n","    lines | 'WriteToBQ' >> beam.io.WriteToBigQuery(project=project, dataset=dataset, table=output, \n","                                                            schema='date:DATE, client:STRING, content:STRING, num:INTEGER, score:FLOAT64, magnitude:FLOAT64',\n","                                                            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n","                                                            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)\n","                                                            # write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE)\n","    # 実行する\n","    q.run().wait_until_finish()\n","\n","def run(argv=None, save_main_session=True):\n","    \"\"\"Main entry point; defines and runs the dataflow pipeline.\"\"\"\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--input', dest='input', required=True, help='Input table')\n","    parser.add_argument('--output', dest='output', required=True, help='Output table')\n","    parser.add_argument('--project', dest='project', required=True, help='project')\n","    parser.add_argument('--region', dest='region', required=True, help='region')\n","    parser.add_argument('--dataset', dest='dataset', required=True, help='dataset')\n","    # parser.add_argument('--client', dest='client', required=True, help='client')\n","    parser.add_argument('--from_date', dest='from_date', required=True, help='From Date(YYYY-MM-DD)')\n","    parser.add_argument('--to_date', dest='to_date', required=True, help='To Date(YYYY-MM-DD)')\n","    parser.add_argument('--bucket', dest='bucket', required=True, help='bucket')\n","\n","    known_args, pipeline_args = parser.parse_known_args(argv)\n","\n","    # PipelineOptionを設定\n","    options = PipelineOptions(pipeline_args)\n","\n","    google_cloud_options = options.view_as(GoogleCloudOptions)\n","    google_cloud_options.project = known_args.project\n","    google_cloud_options.region = known_args.region\n","    google_cloud_options.staging_location = '{}/code/'.format(known_args.bucket)\n","    google_cloud_options.temp_location = '{}/temp/'.format(known_args.bucket)\n","\n","    worker_options = options.view_as(WorkerOptions)\n","    worker_options.disk_size_gb = 30\n","    worker_options.max_num_workers = 1\n","    worker_options.machine_type = 'n1-standard-2'\n","    # worker_options.use_public_ips = True\n","\n","    options.view_as(StandardOptions).runner = 'DataflowRunner'\n","    setup_option = options.view_as(SetupOptions)\n","    setup_option.requirements_file = \"./requirements.txt\"\n","    setup_option.view_as(SetupOptions).setup_file = './setup.py'\n","\n","    # BQからBQ\n","    pipeline(options, known_args)\n","\n","if __name__ == '__main__':\n","    logging.getLogger().setLevel(logging.INFO)\n","    run(sys.argv)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1tr6PdBzc-FM","executionInfo":{"status":"ok","timestamp":1648622837939,"user_tz":-540,"elapsed":13,"user":{"displayName":"Support Product","userId":"10938393275403308243"}},"outputId":"090f23cc-b459-416d-db7e-5517189b6315"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing dataflow_whatya_v2_zozo.py\n"]}]},{"cell_type":"markdown","source":["### oksky_chat_all"],"metadata":{"id":"qGaV0f6-tUq9"}},{"cell_type":"code","source":["%%writefile dataflow_oksky_chat_all.py\n","import argparse\n","import logging\n","import sys\n","from datetime import datetime\n","from apache_beam.io.gcp import gcsio\n","import apache_beam as beam\n","from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions, SetupOptions, WorkerOptions\n","\n","class AnalyzeSentiment(beam.DoFn):\n","    def process(self, context):\n","        import time\n","        from google.cloud import language\n","        from google.cloud.language import enums\n","        from google.cloud.language import types\n","\n","        client = language.LanguageServiceClient()\n","\n","        text = context[\"content\"]\n","        language = \"ja\"\n","        document = types.Document(\n","            content=text,\n","            type=enums.Document.Type.PLAIN_TEXT,\n","            language=language)\n","    \n","        sentiment = client.analyze_sentiment(document).document_sentiment\n","\n","        num = context[\"num\"]\n","        score = sentiment.score\n","        magnitude = sentiment.magnitude\n","\n","        time.sleep(0.1)\n","\n","        return [{'date': context[\"date\"],\n","                 'client': context[\"client\"],\n","                 'content': text,\n","                 'num': num,\n","                 'score': score,\n","                 'magnitude': magnitude\n","                }]\n","\n","def pipeline(options=None, known_args=None):\n","    \"\"\"\n","    BQにクエリを投げ、別のテーブルに出力するパイプライン\n","\n","    :param options:\n","    :param known_args:\n","    :return:\n","    \"\"\"\n","    project = options.view_as(GoogleCloudOptions).project\n","    dataset = known_args.dataset\n","    input = known_args.input\n","    output = known_args.output\n","    # client = known_args.client\n","    from_date = known_args.from_date\n","    to_date = known_args.to_date\n","\n","    query = \"\"\"\n","SELECT\n","  date,\n","  client,\n","  content,\n","  num\n","FROM {}.{}.{}\n","WHERE\n","  date BETWEEN DATE('{}', \"Asia/Tokyo\") AND DATE('{}', \"Asia/Tokyo\")\n","ORDER BY num DESC\n","\"\"\".format(project, dataset, input, from_date, to_date)\n","\n","    ## 新たなパイプラインを生成\n","    q = beam.Pipeline(options=options)\n","\n","    # クエリを実行\n","    #lines = q | 'ReadFromBigQuery' >> beam.io.Read(beam.io.BigQuerySource(query=query, use_standard_sql=True))\n","    lines = q | 'ReadFromBigQuery' >> beam.io.Read(beam.io.gcp.bigquery.ReadFromBigQuery(query=query, use_standard_sql=True))\n","\n","    lines = lines | 'Analyze Sentiment' >> beam.ParDo(AnalyzeSentiment())\n","\n","    # 別のテーブルに書き込む\n","    lines | 'WriteToBQ' >> beam.io.WriteToBigQuery(project=project, dataset=dataset, table=output, \n","                                                            schema='date:DATE, client:STRING, content:STRING, num:INTEGER, score:FLOAT64, magnitude:FLOAT64',\n","                                                            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n","                                                            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)\n","                                                            # write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE)\n","    # 実行する\n","    q.run().wait_until_finish()\n","\n","def run(argv=None, save_main_session=True):\n","    \"\"\"Main entry point; defines and runs the dataflow pipeline.\"\"\"\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--input', dest='input', required=True, help='Input table')\n","    parser.add_argument('--output', dest='output', required=True, help='Output table')\n","    parser.add_argument('--project', dest='project', required=True, help='project')\n","    parser.add_argument('--region', dest='region', required=True, help='region')\n","    parser.add_argument('--dataset', dest='dataset', required=True, help='dataset')\n","    # parser.add_argument('--client', dest='client', required=True, help='client')\n","    parser.add_argument('--from_date', dest='from_date', required=True, help='From Date(YYYY-MM-DD)')\n","    parser.add_argument('--to_date', dest='to_date', required=True, help='To Date(YYYY-MM-DD)')\n","    parser.add_argument('--bucket', dest='bucket', required=True, help='bucket')\n","\n","    known_args, pipeline_args = parser.parse_known_args(argv)\n","\n","    # PipelineOptionを設定\n","    options = PipelineOptions(pipeline_args)\n","\n","    google_cloud_options = options.view_as(GoogleCloudOptions)\n","    google_cloud_options.project = known_args.project\n","    google_cloud_options.region = known_args.region\n","    google_cloud_options.staging_location = '{}/code/'.format(known_args.bucket)\n","    google_cloud_options.temp_location = '{}/temp/'.format(known_args.bucket)\n","\n","    worker_options = options.view_as(WorkerOptions)\n","    worker_options.disk_size_gb = 30\n","    worker_options.max_num_workers = 1\n","    worker_options.machine_type = 'n1-standard-2'\n","    # worker_options.use_public_ips = True\n","\n","    options.view_as(StandardOptions).runner = 'DataflowRunner'\n","    setup_option = options.view_as(SetupOptions)\n","    setup_option.requirements_file = \"./requirements.txt\"\n","    setup_option.view_as(SetupOptions).setup_file = './setup.py'\n","\n","    # BQからBQ\n","    pipeline(options, known_args)\n","\n","if __name__ == '__main__':\n","    logging.getLogger().setLevel(logging.INFO)\n","    run(sys.argv)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y1a4LbChdozx","executionInfo":{"status":"ok","timestamp":1648622837939,"user_tz":-540,"elapsed":11,"user":{"displayName":"Support Product","userId":"10938393275403308243"}},"outputId":"72fff168-d4e6-470d-8e32-d25815d8e7ed"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing dataflow_oksky_chat_all.py\n"]}]},{"cell_type":"markdown","source":["### whatya_v1_kosfja"],"metadata":{"id":"7-PHeM34ta0N"}},{"cell_type":"code","source":["%%writefile dataflow_whatya_v1_kosfja.py\n","import argparse\n","import logging\n","import sys\n","from datetime import datetime\n","from apache_beam.io.gcp import gcsio\n","import apache_beam as beam\n","from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions, SetupOptions, WorkerOptions\n","\n","class AnalyzeSentiment(beam.DoFn):\n","    def process(self, context):\n","        import time\n","        from google.cloud import language\n","        from google.cloud.language import enums\n","        from google.cloud.language import types\n","        client = language.LanguageServiceClient()\n","\n","        text = context[\"content\"]\n","        language = \"ja\"\n","        document = types.Document(\n","            content=text,\n","            type=enums.Document.Type.PLAIN_TEXT,\n","            language=language)\n","    \n","        sentiment = client.analyze_sentiment(document).document_sentiment\n","\n","        num = context[\"num\"]\n","        score = sentiment.score\n","        magnitude = sentiment.magnitude\n","\n","        time.sleep(0.1)\n","\n","        return [{'date': context[\"date\"],\n","                 'client': context[\"client\"],\n","                 'content': text,\n","                 'num': num,\n","                 'score': score,\n","                 'magnitude': magnitude\n","                }]\n","\n","def pipeline(options=None, known_args=None):\n","    \"\"\"\n","    BQにクエリを投げ、別のテーブルに出力するパイプライン\n","\n","    :param options:\n","    :param known_args:\n","    :return:\n","    \"\"\"\n","    project = options.view_as(GoogleCloudOptions).project\n","    dataset = known_args.dataset\n","    input = known_args.input\n","    output = known_args.output\n","    client = known_args.client\n","    from_date = known_args.from_date\n","    to_date = known_args.to_date\n","\n","    query = \"\"\"\n","SELECT\n","  date,\n","  client,\n","  quest_item_name AS content,\n","  count(*) as num\n","from {}.{}.{}\n","where\n","  (client = '{}' AND date BETWEEN DATE('{}', \"Asia/Tokyo\") AND DATE('{}', \"Asia/Tokyo\"))\n","  AND quest_item_value = ''\n","  AND regexp_extract(quest_item_name, '(init|init_bot|テスト|TEST|使い方|肌チェック|完了|肌診断終了|肌終了|肌id AIチャットボットに質問|^[1-9].*|^https?://|^line://|^[a-z|A−Z|あ-ん|0-9|０−９]$)') IS NULL\n","group by date, client, quest_item_name\n","order by num desc\n","\"\"\".format(project, dataset, input, client, from_date, to_date)\n","\n","    ## 新たなパイプラインを生成\n","    q = beam.Pipeline(options=options)\n","\n","    # クエリを実行\n","    #lines = q | 'ReadFromBigQuery' >> beam.io.Read(beam.io.BigQuerySource(query=query, use_standard_sql=True))\n","    lines = q | 'ReadFromBigQuery' >> beam.io.Read(beam.io.gcp.bigquery.ReadFromBigQuery(query=query, use_standard_sql=True))\n","\n","    lines = lines | 'Analyze Sentiment' >> beam.ParDo(AnalyzeSentiment())\n","\n","    # 別のテーブルに書き込む\n","    lines | 'WriteToBQ' >> beam.io.WriteToBigQuery(project=project, dataset=dataset, table=output, \n","                                                            schema='date:DATE, client:STRING, content:STRING, num:INTEGER, score:FLOAT64, magnitude:FLOAT64',\n","                                                            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n","                                                            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)\n","                                                            # write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE)\n","    # 実行する\n","    q.run().wait_until_finish()\n","\n","def run(argv=None, save_main_session=True):\n","    \"\"\"Main entry point; defines and runs the dataflow pipeline.\"\"\"\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--input', dest='input', required=True, help='Input table')\n","    parser.add_argument('--output', dest='output', required=True, help='Output table')\n","    parser.add_argument('--project', dest='project', required=True, help='project')\n","    parser.add_argument('--region', dest='region', required=True, help='region')\n","    parser.add_argument('--dataset', dest='dataset', required=True, help='dataset')\n","    parser.add_argument('--client', dest='client', required=True, help='client')\n","    parser.add_argument('--from_date', dest='from_date', required=True, help='From Date(YYYY-MM-DD)')\n","    parser.add_argument('--to_date', dest='to_date', required=True, help='To Date(YYYY-MM-DD)')\n","    parser.add_argument('--bucket', dest='bucket', required=True, help='bucket')\n","\n","    known_args, pipeline_args = parser.parse_known_args(argv)\n","\n","    # PipelineOptionを設定\n","    options = PipelineOptions(pipeline_args)\n","\n","    google_cloud_options = options.view_as(GoogleCloudOptions)\n","    google_cloud_options.project = known_args.project\n","    google_cloud_options.region = known_args.region\n","    google_cloud_options.staging_location = '{}/code/'.format(known_args.bucket)\n","    google_cloud_options.temp_location = '{}/temp/'.format(known_args.bucket)\n","\n","    worker_options = options.view_as(WorkerOptions)\n","    worker_options.disk_size_gb = 30\n","    worker_options.max_num_workers = 1\n","    worker_options.machine_type = 'n1-standard-2'\n","    # worker_options.use_public_ips = True\n","\n","    options.view_as(StandardOptions).runner = 'DataflowRunner'\n","    setup_option = options.view_as(SetupOptions)\n","    setup_option.requirements_file = \"./requirements.txt\"\n","    setup_option.view_as(SetupOptions).setup_file = './setup.py'\n","\n","    # BQからBQ\n","    pipeline(options, known_args)\n","\n","if __name__ == '__main__':\n","    logging.getLogger().setLevel(logging.INFO)\n","    run(sys.argv)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VflwXYtddiBY","executionInfo":{"status":"ok","timestamp":1648622837940,"user_tz":-540,"elapsed":11,"user":{"displayName":"Support Product","userId":"10938393275403308243"}},"outputId":"2639b193-0988-4b4b-f6a9-7280f0fbe3a0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing dataflow_whatya_v1_kosfja.py\n"]}]},{"cell_type":"code","source":["!ls -l"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VvOdWeSPcaSn","executionInfo":{"status":"ok","timestamp":1648622837940,"user_tz":-540,"elapsed":9,"user":{"displayName":"Support Product","userId":"10938393275403308243"}},"outputId":"6ccb3356-0cd2-4b3f-b31a-f2868e11730a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["total 56\n","-rw-r--r-- 1 root root  720 Mar 30 15:47 adc.json\n","-rw-r--r-- 1 root root 5017 Mar 30 15:47 dataflow_oksky_chat_all.py\n","-rw-r--r-- 1 root root 5381 Mar 30 15:47 dataflow_whatya_v1_kosfja.py\n","-rw-r--r-- 1 root root 5577 Mar 30 15:47 dataflow_whatya_v2_all.py\n","-rw-r--r-- 1 root root 5573 Mar 30 15:47 dataflow_whatya_v2_wlp.py\n","-rw-r--r-- 1 root root 5585 Mar 30 15:47 dataflow_whatya_v2_zozo.py\n","-rw-r--r-- 1 root root    0 Mar 30 15:47 requirements.txt\n","drwxr-xr-x 1 root root 4096 Mar 23 23:22 sample_data\n","-rw-r--r-- 1 root root 5740 Mar 30 15:47 setup.py\n"]}]},{"cell_type":"markdown","source":["## GCP環境設定"],"metadata":{"id":"wrZ2qgiftj1h"}},{"cell_type":"code","source":["import os\n","os.environ[\"GCLOUD_PROJECT\"] = \"bwing-230309\"\n","#os.environ[\"GCLOUD_PROJECT\"] = \"learnlearn-208609\""],"metadata":{"id":"vlgwBAaGjnSR","executionInfo":{"status":"ok","timestamp":1648622837940,"user_tz":-540,"elapsed":7,"user":{"displayName":"Support Product","userId":"10938393275403308243"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["!gcloud config set project bwing-230309"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dHgu4qBhqIFp","executionInfo":{"status":"ok","timestamp":1648622839490,"user_tz":-540,"elapsed":1557,"user":{"displayName":"Support Product","userId":"10938393275403308243"}},"outputId":"130f3a97-c0eb-4759-c857-4c5ed8d9b4da"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated property [core/project].\n"]}]},{"cell_type":"code","source":["#!env"],"metadata":{"id":"Wg8hxenRmhYq","executionInfo":{"status":"ok","timestamp":1648622839491,"user_tz":-540,"elapsed":4,"user":{"displayName":"Support Product","userId":"10938393275403308243"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["## Dataflow実行"],"metadata":{"id":"jucQfS5vtsLW"}},{"cell_type":"code","source":["#!python dataflow_whatya_v2_all.py --project=bwing-230309 --region=asia-east1 --dataset=whatya --input=log_info_v2 --output=sentiment_test --from_date=2022-02-01 --to_date=2022-02-02 --bucket=gs://bwing-230309_dataflow"],"metadata":{"id":"IjFNwTO9gzOT","executionInfo":{"status":"ok","timestamp":1648622839491,"user_tz":-540,"elapsed":3,"user":{"displayName":"Support Product","userId":"10938393275403308243"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["%%bash\n","### 今月分\n","#export FROM_DATE=$(date +\"%Y-%m-%d\" -d\"`date +\"%Y%m01\"`\")\n","#export TO_DATE=$(date +\"%Y-%m-%d\" -d\"`date +\"%Y%m01\"` 1 days ago + 1 month\")\n","\n","### 先月分\n","export FROM_DATE=$(date +\"%Y-%m-%d\" -d\"`date +\"%Y%m01\"` 1 month ago\")\n","export TO_DATE=$(date +\"%Y-%m-%d\" -d\"`date +\"%Y%m01\"` 1 days ago\")\n","\n","echo ${FROM_DATE}\n","echo ${TO_DATE}\n","\n","export PROJECT=bwing-230309\n","export REGION=asia-east1\n","#export DATASET=whatya\n","#export INPUT=log_info\n","export OUTPUT=sentiment\n","#export OUTPUT=sentiment_test\n","\n","# BigQuery のデータのロケーション（asia-northeast1）とGCSバケットの場所が一致すること\n","export BUCKET=gs://bwing-230309_dataflow\n","\n","# WhatYa v2\n","python dataflow_whatya_v2_all.py --project=${PROJECT} --region=${REGION} --dataset=whatya --input=log_info_v2 --output=${OUTPUT} --from_date=${FROM_DATE} --to_date=${TO_DATE} --bucket=${BUCKET}\n","\n","# WLP\n","python3 dataflow_whatya_v2_wlp.py --project=${PROJECT} --region=${REGION} --dataset=whatya_smb --input=log_info_v2 --output=${OUTPUT}  --from_date=${FROM_DATE} --to_date=${TO_DATE} --bucket=${BUCKET}\n","\n","# zozo\n","python3 dataflow_whatya_v2_zozo.py --project=${PROJECT} --region=${REGION} --dataset=whatya --input=log_info_v2 --output=${OUTPUT}  --from_date=${FROM_DATE} --to_date=${TO_DATE} --bucket=${BUCKET}\n","\n","# OKSKY\n","python3 dataflow_oksky_chat_all.py --project=${PROJECT} --region=${REGION} --dataset=oksky_chat --input=quest --output=${OUTPUT}  --from_date=${FROM_DATE} --to_date=${TO_DATE} --bucket=${BUCKET}\n","\n","# WhatYa v1 (kosfja)\n","python3 dataflow_whatya_v1_kosfja.py --project=${PROJECT} --region=${REGION} --dataset=whatya --input=log_info --output=${OUTPUT}  --client=kosfja --from_date=${FROM_DATE} --to_date=${TO_DATE} --bucket=${BUCKET}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WvlDg1sDZyh2","executionInfo":{"status":"ok","timestamp":1648627354380,"user_tz":-540,"elapsed":145977,"user":{"displayName":"Support Product","userId":"10938393275403308243"}},"outputId":"8afec404-233a-4b30-c3f6-305645e199d4"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-02-01\n","2022-02-28\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/apache_beam/io/gcp/bigquery.py:2437: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n","  temp_location = pcoll.pipeline.options.view_as(\n","/usr/local/lib/python3.7/dist-packages/apache_beam/io/gcp/bigquery_file_loads.py:1128: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n","  temp_location = p.options.view_as(GoogleCloudOptions).temp_location\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/dataflow-requirements-cache', '-r', '/tmp/tmpnsffgsnm/tmp_requirements.txt', '--exists-action', 'i', '--no-binary', ':all:']\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmpsqhcd_wd']\n","warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n","\n","INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpsqhcd_wd', 'apache-beam==2.37.0', '--no-deps', '--no-binary', ':all:']\n","INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n","INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpsqhcd_wd', 'apache-beam==2.37.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n","INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl\n","WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n","INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.37.0\n","INFO:root:Using provided Python SDK container image: gcr.io/cloud-dataflow/v1beta3/python37:2.37.0\n","INFO:root:Python SDK container image set to \"gcr.io/cloud-dataflow/v1beta3/python37:2.37.0\" for Docker environment\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f82169dce60> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f82169de680> ====================\n","INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n","INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n","INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n","INFO:oauth2client.client:Refreshing access_token\n","INFO:apache_beam.io.gcp.bigquery_tools:Started BigQuery job: <JobReference\n"," location: 'asia-northeast1'\n"," projectId: 'bwing-230309'>\n"," bq show -j --format=prettyjson --project_id=bwing-230309 None\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330064744-198769-8kmf4qr5.1648622864.198951/requirements.txt...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330064744-198769-8kmf4qr5.1648622864.198951/requirements.txt in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330064744-198769-8kmf4qr5.1648622864.198951/dataflow_python_sdk.tar...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330064744-198769-8kmf4qr5.1648622864.198951/dataflow_python_sdk.tar in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330064744-198769-8kmf4qr5.1648622864.198951/workflow.tar.gz...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330064744-198769-8kmf4qr5.1648622864.198951/workflow.tar.gz in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330064744-198769-8kmf4qr5.1648622864.198951/apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330064744-198769-8kmf4qr5.1648622864.198951/apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl in 6 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330064744-198769-8kmf4qr5.1648622864.198951/pipeline.pb...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330064744-198769-8kmf4qr5.1648622864.198951/pipeline.pb in 0 seconds.\n","WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['dataflow_whatya_v2_all.py']\n","WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['dataflow_whatya_v2_all.py']\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n"," clientRequestId: '20220330064744200055-7566'\n"," createTime: '2022-03-30T06:47:57.158736Z'\n"," currentStateTime: '1970-01-01T00:00:00Z'\n"," id: '2022-03-29_23_47_56-555239502999271058'\n"," location: 'asia-east1'\n"," name: 'beamapp-root-0330064744-198769-8kmf4qr5'\n"," projectId: 'bwing-230309'\n"," stageStates: []\n"," startTime: '2022-03-30T06:47:57.158736Z'\n"," steps: []\n"," tempFiles: []\n"," type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2022-03-29_23_47_56-555239502999271058]\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2022-03-29_23_47_56-555239502999271058\n","INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/asia-east1/2022-03-29_23_47_56-555239502999271058?project=bwing-230309\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-29_23_47_56-555239502999271058 is in state JOB_STATE_PENDING\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:47:57.305Z: JOB_MESSAGE_BASIC: The pipeline is using shuffle service with a (boot) persistent disk size / type other than the default. If that configuration was intended solely to speed up the non-service shuffle, consider removing it to reduce costs as those disks are unused by the shuffle service.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:47:57.726Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2022-03-29_23_47_56-555239502999271058. The number of workers will be between 1 and 1.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:47:57.760Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2022-03-29_23_47_56-555239502999271058.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:00.120Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-2 in asia-east1-c.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.215Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.249Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.277Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.309Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.354Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.388Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.536Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.590Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.622Z: JOB_MESSAGE_DETAILED: Unzipping flatten s25 for input s19.WrittenFiles\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.656Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteToBQ/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.682Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.716Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.744Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.767Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.803Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.900Z: JOB_MESSAGE_DETAILED: Unzipping flatten s25-u32 for input s26.None-c30\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.936Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteToBQ/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.974Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround into WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:01.999Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.040Z: JOB_MESSAGE_DETAILED: Fusing consumer ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough) into ReadFromBigQuery/ReadFromBigQuery/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.066Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze Sentiment into ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.101Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal into Analyze Sentiment\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.120Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/AppendDestination into WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.177Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteToBQ/BigQueryBatchFileLoads/AppendDestination\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.205Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.232Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.267Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.371Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.401Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/DropShardNumber into WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.434Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteToBQ/BigQueryBatchFileLoads/DropShardNumber\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.489Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.523Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.560Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.590Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.618Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/ParDo(UpdateDestinationSchema) into WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-29_23_47_56-555239502999271058 is in state JOB_STATE_RUNNING\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.653Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.677Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/WaitForSchemaModJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.701Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.737Z: JOB_MESSAGE_DETAILED: Fusing consumer ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove into ReadFromBigQuery/ReadFromBigQuery/FilesToRemoveImpulse/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.765Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.792Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.819Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.859Z: JOB_MESSAGE_DETAILED: Fusing consumer ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles) into ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/Create/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.897Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.924Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.955Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:02.990Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:03.026Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:03.066Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:03.098Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:03.124Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:03.181Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:03.340Z: JOB_MESSAGE_DEBUG: Executing wait step start52\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:03.395Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/FilesToRemoveImpulse/Read+ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:03.420Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:03.433Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:03.453Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:03.488Z: JOB_MESSAGE_BASIC: Starting 1 workers in asia-east1-c...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:03.505Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:03.566Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:48:50.856Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:49:11.817Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:08.802Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/FilesToRemoveImpulse/Read+ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:08.839Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:08.874Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:09.055Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:09.093Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.079Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.118Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.134Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.150Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.167Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.184Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.200Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.217Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/_UnpickledSideInput(SchemaModJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.235Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.247Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.250Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.263Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.265Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.286Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.302Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/_UnpickledSideInput(SchemaModJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.306Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.311Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.364Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.406Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/_UnpickledSideInput(SchemaModJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.408Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.423Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.440Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.457Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.475Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.814Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.855Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:10.892Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:11.080Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:11.114Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T06:53:11.153Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)+Analyze Sentiment+WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteToBQ/BigQueryBatchFileLoads/AppendDestination+WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:41.950Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)+Analyze Sentiment+WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteToBQ/BigQueryBatchFileLoads/AppendDestination+WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:42.047Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough).cleanup_signal\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:42.082Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:42.118Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:42.150Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:42.201Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:42.204Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/DropShardNumber+WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:42.277Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:42.337Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/Create/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:43.987Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/DropShardNumber+WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:44.078Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:44.146Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:44.228Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:45.640Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/Create/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.343Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.425Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.470Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.496Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.538Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.581Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.615Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.642Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.651Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/Flatten\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.713Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.761Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.786Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/ParDo(UpdateDestinationSchema)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.792Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.830Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.896Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/Flatten\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.897Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.964Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/Flatten.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:51.987Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:55.412Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:58.682Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/ParDo(UpdateDestinationSchema)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:58.758Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:58.796Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema).out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:58.889Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/_UnpickledSideInput(ParDo(UpdateDestinationSchema).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:59.078Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/_UnpickledSideInput(ParDo(UpdateDestinationSchema).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:59.158Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/_UnpickledSideInput(ParDo(UpdateDestinationSchema).out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:04:59.245Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/WaitForSchemaModJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:02.699Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/WaitForSchemaModJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:02.760Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:02.832Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(WaitForSchemaModJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:03.040Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(WaitForSchemaModJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:03.101Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(WaitForSchemaModJobs.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:03.160Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:08.160Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:08.234Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:08.294Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:08.369Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:08.435Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:08.511Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:11.528Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:11.588Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:11.656Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(WaitForCopyJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:11.729Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(WaitForCopyJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:11.800Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(WaitForCopyJobs.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:11.890Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:12.052Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:12.119Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:12.187Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:13.652Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:13.731Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:13.778Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:13.846Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Delete\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:14.152Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Delete\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:14.231Z: JOB_MESSAGE_DEBUG: Executing success step success50\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:14.323Z: JOB_MESSAGE_DETAILED: Cleaning up.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:14.367Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:05:14.395Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:07:35.433Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:07:35.476Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:07:35.493Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-29_23_47_56-555239502999271058 is in state JOB_STATE_DONE\n","/usr/local/lib/python3.7/dist-packages/apache_beam/io/gcp/bigquery.py:2437: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n","  temp_location = pcoll.pipeline.options.view_as(\n","/usr/local/lib/python3.7/dist-packages/apache_beam/io/gcp/bigquery_file_loads.py:1128: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n","  temp_location = p.options.view_as(GoogleCloudOptions).temp_location\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/dataflow-requirements-cache', '-r', '/tmp/tmp_pg7d49d/tmp_requirements.txt', '--exists-action', 'i', '--no-binary', ':all:']\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmpr1534msn']\n","warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n","\n","INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpr1534msn', 'apache-beam==2.37.0', '--no-deps', '--no-binary', ':all:']\n","INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n","INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpr1534msn', 'apache-beam==2.37.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n","INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl\n","WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n","INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.37.0\n","INFO:root:Using provided Python SDK container image: gcr.io/cloud-dataflow/v1beta3/python37:2.37.0\n","INFO:root:Python SDK container image set to \"gcr.io/cloud-dataflow/v1beta3/python37:2.37.0\" for Docker environment\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fcc42bdbe60> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fcc42bde680> ====================\n","INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n","INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n","INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n","INFO:oauth2client.client:Refreshing access_token\n","INFO:apache_beam.io.gcp.bigquery_tools:Started BigQuery job: <JobReference\n"," location: 'asia-northeast1'\n"," projectId: 'bwing-230309'>\n"," bq show -j --format=prettyjson --project_id=bwing-230309 None\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330070800-790293-bpfp5fkx.1648624080.790538/requirements.txt...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330070800-790293-bpfp5fkx.1648624080.790538/requirements.txt in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330070800-790293-bpfp5fkx.1648624080.790538/dataflow_python_sdk.tar...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330070800-790293-bpfp5fkx.1648624080.790538/dataflow_python_sdk.tar in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330070800-790293-bpfp5fkx.1648624080.790538/workflow.tar.gz...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330070800-790293-bpfp5fkx.1648624080.790538/workflow.tar.gz in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330070800-790293-bpfp5fkx.1648624080.790538/apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330070800-790293-bpfp5fkx.1648624080.790538/apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl in 5 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330070800-790293-bpfp5fkx.1648624080.790538/pipeline.pb...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330070800-790293-bpfp5fkx.1648624080.790538/pipeline.pb in 0 seconds.\n","WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['dataflow_whatya_v2_wlp.py']\n","WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['dataflow_whatya_v2_wlp.py']\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n"," clientRequestId: '20220330070800791740-3203'\n"," createTime: '2022-03-30T07:08:13.116835Z'\n"," currentStateTime: '1970-01-01T00:00:00Z'\n"," id: '2022-03-30_00_08_12-16220988496165242455'\n"," location: 'asia-east1'\n"," name: 'beamapp-root-0330070800-790293-bpfp5fkx'\n"," projectId: 'bwing-230309'\n"," stageStates: []\n"," startTime: '2022-03-30T07:08:13.116835Z'\n"," steps: []\n"," tempFiles: []\n"," type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2022-03-30_00_08_12-16220988496165242455]\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2022-03-30_00_08_12-16220988496165242455\n","INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/asia-east1/2022-03-30_00_08_12-16220988496165242455?project=bwing-230309\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-30_00_08_12-16220988496165242455 is in state JOB_STATE_PENDING\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:13.262Z: JOB_MESSAGE_BASIC: The pipeline is using shuffle service with a (boot) persistent disk size / type other than the default. If that configuration was intended solely to speed up the non-service shuffle, consider removing it to reduce costs as those disks are unused by the shuffle service.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:13.668Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2022-03-30_00_08_12-16220988496165242455. The number of workers will be between 1 and 1.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:13.685Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2022-03-30_00_08_12-16220988496165242455.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:15.772Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-2 in asia-east1-c.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:16.923Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:16.972Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:16.993Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.011Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.036Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.076Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.144Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.178Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.195Z: JOB_MESSAGE_DETAILED: Unzipping flatten s25 for input s19.WrittenFiles\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.211Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteToBQ/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.225Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.241Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.256Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.271Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.288Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.302Z: JOB_MESSAGE_DETAILED: Unzipping flatten s25-u32 for input s26.None-c30\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.317Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteToBQ/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.332Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround into WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.348Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.365Z: JOB_MESSAGE_DETAILED: Fusing consumer ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough) into ReadFromBigQuery/ReadFromBigQuery/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.381Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze Sentiment into ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.413Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal into Analyze Sentiment\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.429Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/AppendDestination into WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.444Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteToBQ/BigQueryBatchFileLoads/AppendDestination\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.488Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.507Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.522Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.537Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.614Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/DropShardNumber into WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.630Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteToBQ/BigQueryBatchFileLoads/DropShardNumber\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.645Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.660Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.675Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.690Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.706Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/ParDo(UpdateDestinationSchema) into WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.722Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.738Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/WaitForSchemaModJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.754Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.769Z: JOB_MESSAGE_DETAILED: Fusing consumer ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove into ReadFromBigQuery/ReadFromBigQuery/FilesToRemoveImpulse/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.785Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.802Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.817Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.832Z: JOB_MESSAGE_DETAILED: Fusing consumer ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles) into ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/Create/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.848Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.864Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.879Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.896Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.911Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.935Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.968Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:17.986Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:18.002Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:18.136Z: JOB_MESSAGE_DEBUG: Executing wait step start52\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:18.169Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/FilesToRemoveImpulse/Read+ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:18.185Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:18.200Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:18.202Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:18.214Z: JOB_MESSAGE_BASIC: Starting 1 workers in asia-east1-c...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:18.218Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:08:18.254Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-30_00_08_12-16220988496165242455 is in state JOB_STATE_RUNNING\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:09:06.618Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:09:29.727Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:29.648Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/FilesToRemoveImpulse/Read+ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:29.688Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:29.727Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:29.804Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:29.843Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.169Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.242Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.258Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.273Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.289Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.311Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.327Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.346Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/_UnpickledSideInput(SchemaModJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.361Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.377Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.387Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.393Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.420Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.430Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.445Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.460Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.465Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.482Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.498Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.498Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.512Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/_UnpickledSideInput(SchemaModJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.536Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.554Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:30.571Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/_UnpickledSideInput(SchemaModJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:32.545Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:32.615Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:32.705Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:32.870Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:32.908Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:13:32.946Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)+Analyze Sentiment+WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteToBQ/BigQueryBatchFileLoads/AppendDestination+WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:14:53.068Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)+Analyze Sentiment+WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteToBQ/BigQueryBatchFileLoads/AppendDestination+WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:14:53.123Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough).cleanup_signal\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:14:53.141Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:14:53.158Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:14:53.183Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:14:53.232Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:14:53.260Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/DropShardNumber+WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:14:53.279Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:14:53.322Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/Create/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:14:55.099Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/DropShardNumber+WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:14:55.140Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:14:55.182Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:14:55.259Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:14:56.322Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/Create/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:01.763Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:01.799Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:01.813Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:01.827Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:01.841Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:01.857Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:01.872Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:01.888Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/Flatten\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:01.913Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:01.922Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:01.934Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:01.944Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:01.960Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:01.976Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:01.992Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/ParDo(UpdateDestinationSchema)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:02.001Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/Flatten\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:02.013Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:02.031Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/Flatten.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:04.288Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:06.202Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/ParDo(UpdateDestinationSchema)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:06.264Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:06.282Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema).out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:06.319Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/_UnpickledSideInput(ParDo(UpdateDestinationSchema).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:06.385Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/_UnpickledSideInput(ParDo(UpdateDestinationSchema).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:06.419Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/_UnpickledSideInput(ParDo(UpdateDestinationSchema).out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:06.454Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/WaitForSchemaModJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:09.857Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/WaitForSchemaModJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:09.891Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:09.923Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(WaitForSchemaModJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:09.990Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(WaitForSchemaModJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:10.025Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(WaitForSchemaModJobs.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:10.062Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:13.698Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:13.742Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:13.775Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:13.840Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:13.875Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:13.914Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:17.247Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:17.288Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:17.323Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(WaitForCopyJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:17.390Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(WaitForCopyJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:17.424Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(WaitForCopyJobs.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:17.461Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:17.607Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:17.644Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:17.727Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:19.805Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:19.838Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:19.882Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:19.917Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Delete\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:20.186Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Delete\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:20.222Z: JOB_MESSAGE_DEBUG: Executing success step success50\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:20.319Z: JOB_MESSAGE_DETAILED: Cleaning up.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:20.357Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:15:20.373Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:17:41.248Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:17:41.276Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:17:41.333Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-30_00_08_12-16220988496165242455 is in state JOB_STATE_DONE\n","/usr/local/lib/python3.7/dist-packages/apache_beam/io/gcp/bigquery.py:2437: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n","  temp_location = pcoll.pipeline.options.view_as(\n","/usr/local/lib/python3.7/dist-packages/apache_beam/io/gcp/bigquery_file_loads.py:1128: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n","  temp_location = p.options.view_as(GoogleCloudOptions).temp_location\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/dataflow-requirements-cache', '-r', '/tmp/tmpz0enbqd_/tmp_requirements.txt', '--exists-action', 'i', '--no-binary', ':all:']\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmplc198hqu']\n","warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n","\n","INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmplc198hqu', 'apache-beam==2.37.0', '--no-deps', '--no-binary', ':all:']\n","INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n","INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmplc198hqu', 'apache-beam==2.37.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n","INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl\n","WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n","INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.37.0\n","INFO:root:Using provided Python SDK container image: gcr.io/cloud-dataflow/v1beta3/python37:2.37.0\n","INFO:root:Python SDK container image set to \"gcr.io/cloud-dataflow/v1beta3/python37:2.37.0\" for Docker environment\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f4ad76a0e60> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f4ad76a4680> ====================\n","INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n","INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n","INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n","INFO:oauth2client.client:Refreshing access_token\n","INFO:apache_beam.io.gcp.bigquery_tools:Started BigQuery job: <JobReference\n"," location: 'asia-northeast1'\n"," projectId: 'bwing-230309'>\n"," bq show -j --format=prettyjson --project_id=bwing-230309 None\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330071811-060314-qrv0ojdl.1648624691.060540/requirements.txt...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330071811-060314-qrv0ojdl.1648624691.060540/requirements.txt in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330071811-060314-qrv0ojdl.1648624691.060540/dataflow_python_sdk.tar...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330071811-060314-qrv0ojdl.1648624691.060540/dataflow_python_sdk.tar in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330071811-060314-qrv0ojdl.1648624691.060540/workflow.tar.gz...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330071811-060314-qrv0ojdl.1648624691.060540/workflow.tar.gz in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330071811-060314-qrv0ojdl.1648624691.060540/apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330071811-060314-qrv0ojdl.1648624691.060540/apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl in 6 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330071811-060314-qrv0ojdl.1648624691.060540/pipeline.pb...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330071811-060314-qrv0ojdl.1648624691.060540/pipeline.pb in 0 seconds.\n","WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['dataflow_whatya_v2_zozo.py']\n","WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['dataflow_whatya_v2_zozo.py']\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n"," clientRequestId: '20220330071811061615-1231'\n"," createTime: '2022-03-30T07:18:24.204969Z'\n"," currentStateTime: '1970-01-01T00:00:00Z'\n"," id: '2022-03-30_00_18_23-66422755425284026'\n"," location: 'asia-east1'\n"," name: 'beamapp-root-0330071811-060314-qrv0ojdl'\n"," projectId: 'bwing-230309'\n"," stageStates: []\n"," startTime: '2022-03-30T07:18:24.204969Z'\n"," steps: []\n"," tempFiles: []\n"," type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2022-03-30_00_18_23-66422755425284026]\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2022-03-30_00_18_23-66422755425284026\n","INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/asia-east1/2022-03-30_00_18_23-66422755425284026?project=bwing-230309\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-30_00_18_23-66422755425284026 is in state JOB_STATE_PENDING\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:24.382Z: JOB_MESSAGE_BASIC: The pipeline is using shuffle service with a (boot) persistent disk size / type other than the default. If that configuration was intended solely to speed up the non-service shuffle, consider removing it to reduce costs as those disks are unused by the shuffle service.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:24.789Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2022-03-30_00_18_23-66422755425284026. The number of workers will be between 1 and 1.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:24.842Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2022-03-30_00_18_23-66422755425284026.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:26.841Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-2 in asia-east1-c.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.091Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.109Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.123Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.137Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.166Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.209Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.273Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.303Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.320Z: JOB_MESSAGE_DETAILED: Unzipping flatten s25 for input s19.WrittenFiles\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.335Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteToBQ/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.351Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.368Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.383Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.399Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.415Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.430Z: JOB_MESSAGE_DETAILED: Unzipping flatten s25-u32 for input s26.None-c30\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.446Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteToBQ/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.463Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround into WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.485Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.501Z: JOB_MESSAGE_DETAILED: Fusing consumer ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough) into ReadFromBigQuery/ReadFromBigQuery/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.516Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze Sentiment into ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.530Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal into Analyze Sentiment\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.547Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/AppendDestination into WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.561Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteToBQ/BigQueryBatchFileLoads/AppendDestination\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.576Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.590Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.607Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.651Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.669Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/DropShardNumber into WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.687Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteToBQ/BigQueryBatchFileLoads/DropShardNumber\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.746Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.762Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.777Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.835Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.850Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/ParDo(UpdateDestinationSchema) into WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.866Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.881Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/WaitForSchemaModJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.897Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.914Z: JOB_MESSAGE_DETAILED: Fusing consumer ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove into ReadFromBigQuery/ReadFromBigQuery/FilesToRemoveImpulse/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.929Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.945Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.962Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.976Z: JOB_MESSAGE_DETAILED: Fusing consumer ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles) into ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/Create/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:28.992Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:29.006Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:29.046Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:29.062Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:29.078Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:29.099Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:29.115Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:29.130Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:29.146Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:29.271Z: JOB_MESSAGE_DEBUG: Executing wait step start52\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:29.304Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/FilesToRemoveImpulse/Read+ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:29.320Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:29.333Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:29.334Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:29.349Z: JOB_MESSAGE_BASIC: Starting 1 workers in asia-east1-c...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:29.351Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:18:29.382Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-30_00_18_23-66422755425284026 is in state JOB_STATE_RUNNING\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:19:20.066Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:19:42.019Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:21.106Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/FilesToRemoveImpulse/Read+ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:21.167Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:21.206Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:21.385Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:21.428Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.070Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.163Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.180Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.210Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.225Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.245Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.260Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.275Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/_UnpickledSideInput(SchemaModJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.311Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.315Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.329Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.344Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.361Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.378Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.410Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.466Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/_UnpickledSideInput(SchemaModJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.468Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.500Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/_UnpickledSideInput(SchemaModJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.501Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.513Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.514Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.558Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.576Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:23.598Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:34.041Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:34.080Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:34.172Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:34.303Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:34.342Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:23:34.384Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)+Analyze Sentiment+WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteToBQ/BigQueryBatchFileLoads/AppendDestination+WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:39.207Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)+Analyze Sentiment+WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteToBQ/BigQueryBatchFileLoads/AppendDestination+WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:39.245Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough).cleanup_signal\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:39.261Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:39.282Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:39.300Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:39.353Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:39.355Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/DropShardNumber+WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:39.404Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:39.446Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/Create/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:39.875Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/DropShardNumber+WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:39.935Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:39.975Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:40.010Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.172Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.218Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.233Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.249Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.267Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.282Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.298Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.330Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.335Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/Flatten\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.363Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.387Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.398Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/ParDo(UpdateDestinationSchema)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.418Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.451Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.451Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.452Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/Flatten\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.479Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.494Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/Flatten.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:46.531Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/Create/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:50.878Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:51.878Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/ParDo(UpdateDestinationSchema)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:51.937Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:51.952Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema).out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:51.989Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/_UnpickledSideInput(ParDo(UpdateDestinationSchema).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:52.055Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/_UnpickledSideInput(ParDo(UpdateDestinationSchema).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:52.088Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/_UnpickledSideInput(ParDo(UpdateDestinationSchema).out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:52.139Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/WaitForSchemaModJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:57.397Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/WaitForSchemaModJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:57.430Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:57.466Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(WaitForSchemaModJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:57.535Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(WaitForSchemaModJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:57.572Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(WaitForSchemaModJobs.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:24:57.604Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:03.262Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:03.296Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:03.348Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:03.419Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:03.452Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:03.505Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:10.137Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:10.207Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:10.238Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(WaitForCopyJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:10.413Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(WaitForCopyJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:10.445Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(WaitForCopyJobs.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:10.484Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:10.617Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:10.651Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:10.688Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:13.078Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:13.114Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:13.159Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:13.198Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Delete\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:13.545Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Delete\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:13.583Z: JOB_MESSAGE_DEBUG: Executing success step success50\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:13.639Z: JOB_MESSAGE_DETAILED: Cleaning up.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:13.682Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:25:13.699Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:27:39.932Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:27:40.019Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:27:40.042Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-30_00_18_23-66422755425284026 is in state JOB_STATE_DONE\n","/usr/local/lib/python3.7/dist-packages/apache_beam/io/gcp/bigquery.py:2437: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n","  temp_location = pcoll.pipeline.options.view_as(\n","/usr/local/lib/python3.7/dist-packages/apache_beam/io/gcp/bigquery_file_loads.py:1128: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n","  temp_location = p.options.view_as(GoogleCloudOptions).temp_location\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/dataflow-requirements-cache', '-r', '/tmp/tmpzsclnr9n/tmp_requirements.txt', '--exists-action', 'i', '--no-binary', ':all:']\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmpwxhklp0q']\n","warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n","\n","INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpwxhklp0q', 'apache-beam==2.37.0', '--no-deps', '--no-binary', ':all:']\n","INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n","INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpwxhklp0q', 'apache-beam==2.37.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n","INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl\n","WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n","INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.37.0\n","INFO:root:Using provided Python SDK container image: gcr.io/cloud-dataflow/v1beta3/python37:2.37.0\n","INFO:root:Python SDK container image set to \"gcr.io/cloud-dataflow/v1beta3/python37:2.37.0\" for Docker environment\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f59c6a19e60> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f59c6a1a680> ====================\n","INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n","INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n","INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n","INFO:oauth2client.client:Refreshing access_token\n","INFO:apache_beam.io.gcp.bigquery_tools:Started BigQuery job: <JobReference\n"," location: 'asia-northeast1'\n"," projectId: 'bwing-230309'>\n"," bq show -j --format=prettyjson --project_id=bwing-230309 None\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330072842-358885-cvq7m7x4.1648625322.359069/requirements.txt...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330072842-358885-cvq7m7x4.1648625322.359069/requirements.txt in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330072842-358885-cvq7m7x4.1648625322.359069/dataflow_python_sdk.tar...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330072842-358885-cvq7m7x4.1648625322.359069/dataflow_python_sdk.tar in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330072842-358885-cvq7m7x4.1648625322.359069/workflow.tar.gz...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330072842-358885-cvq7m7x4.1648625322.359069/workflow.tar.gz in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330072842-358885-cvq7m7x4.1648625322.359069/apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330072842-358885-cvq7m7x4.1648625322.359069/apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl in 6 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330072842-358885-cvq7m7x4.1648625322.359069/pipeline.pb...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330072842-358885-cvq7m7x4.1648625322.359069/pipeline.pb in 0 seconds.\n","WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['dataflow_oksky_chat_all.py']\n","WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['dataflow_oksky_chat_all.py']\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n"," clientRequestId: '20220330072842360165-2068'\n"," createTime: '2022-03-30T07:28:56.004958Z'\n"," currentStateTime: '1970-01-01T00:00:00Z'\n"," id: '2022-03-30_00_28_55-10446954811771525401'\n"," location: 'asia-east1'\n"," name: 'beamapp-root-0330072842-358885-cvq7m7x4'\n"," projectId: 'bwing-230309'\n"," stageStates: []\n"," startTime: '2022-03-30T07:28:56.004958Z'\n"," steps: []\n"," tempFiles: []\n"," type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2022-03-30_00_28_55-10446954811771525401]\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2022-03-30_00_28_55-10446954811771525401\n","INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/asia-east1/2022-03-30_00_28_55-10446954811771525401?project=bwing-230309\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-30_00_28_55-10446954811771525401 is in state JOB_STATE_PENDING\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:28:56.187Z: JOB_MESSAGE_BASIC: The pipeline is using shuffle service with a (boot) persistent disk size / type other than the default. If that configuration was intended solely to speed up the non-service shuffle, consider removing it to reduce costs as those disks are unused by the shuffle service.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:28:56.589Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2022-03-30_00_28_55-10446954811771525401. The number of workers will be between 1 and 1.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:28:56.604Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2022-03-30_00_28_55-10446954811771525401.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:28:58.468Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-2 in asia-east1-c.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:28:59.682Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:28:59.704Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:28:59.719Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:28:59.735Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:28:59.764Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:28:59.783Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:28:59.930Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:28:59.963Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:28:59.980Z: JOB_MESSAGE_DETAILED: Unzipping flatten s25 for input s19.WrittenFiles\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:28:59.998Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteToBQ/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.015Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.032Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.049Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.066Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.085Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.100Z: JOB_MESSAGE_DETAILED: Unzipping flatten s25-u32 for input s26.None-c30\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.118Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteToBQ/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.134Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround into WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.152Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.220Z: JOB_MESSAGE_DETAILED: Fusing consumer ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough) into ReadFromBigQuery/ReadFromBigQuery/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.238Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze Sentiment into ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.254Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal into Analyze Sentiment\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.285Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/AppendDestination into WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.301Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteToBQ/BigQueryBatchFileLoads/AppendDestination\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.316Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.332Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.350Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.366Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.383Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/DropShardNumber into WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.431Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteToBQ/BigQueryBatchFileLoads/DropShardNumber\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.448Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.466Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.481Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.500Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.538Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/ParDo(UpdateDestinationSchema) into WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.554Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.572Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/WaitForSchemaModJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.588Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.604Z: JOB_MESSAGE_DETAILED: Fusing consumer ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove into ReadFromBigQuery/ReadFromBigQuery/FilesToRemoveImpulse/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.621Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.638Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.655Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.673Z: JOB_MESSAGE_DETAILED: Fusing consumer ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles) into ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/Create/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.689Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.706Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.724Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.741Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.759Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.784Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.803Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.837Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:00.854Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:01.050Z: JOB_MESSAGE_DEBUG: Executing wait step start52\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:01.086Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/FilesToRemoveImpulse/Read+ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:01.104Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:01.118Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:01.120Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:01.135Z: JOB_MESSAGE_BASIC: Starting 1 workers in asia-east1-c...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:01.139Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:01.237Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-30_00_28_55-10446954811771525401 is in state JOB_STATE_RUNNING\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:29:54.270Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:30:15.744Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:09.146Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/FilesToRemoveImpulse/Read+ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:09.184Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:09.227Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:09.449Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:09.485Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:10.707Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:10.744Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:10.760Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:10.776Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:10.794Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:10.810Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:10.828Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:10.844Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/_UnpickledSideInput(SchemaModJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:10.860Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:10.977Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:10.987Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:10.993Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:10.997Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:11.019Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:11.022Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/_UnpickledSideInput(SchemaModJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:11.030Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:11.036Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:11.052Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/_UnpickledSideInput(SchemaModJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:11.068Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:11.144Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:11.157Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:11.176Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:11.193Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:11.227Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:11.740Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:11.777Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:11.814Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:11.947Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:11.995Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:34:12.031Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)+Analyze Sentiment+WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteToBQ/BigQueryBatchFileLoads/AppendDestination+WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:07.172Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)+Analyze Sentiment+WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteToBQ/BigQueryBatchFileLoads/AppendDestination+WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:07.215Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough).cleanup_signal\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:07.230Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:07.258Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:07.269Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:07.309Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/DropShardNumber+WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:07.430Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:07.470Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:07.506Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/Create/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:08.963Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/DropShardNumber+WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:09Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:09.046Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:09.127Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:10.611Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/Create/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:14.956Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:14.995Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:15.010Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:15.025Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:15.105Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:15.121Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:15.183Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:15.199Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/Flatten\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:15.273Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:15.284Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:15.314Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:15.333Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:15.349Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:15.353Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/ParDo(UpdateDestinationSchema)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:15.385Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:15.479Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:15.541Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/Flatten\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:15.573Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/Flatten.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:16.504Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:23.045Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/ParDo(UpdateDestinationSchema)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:23.148Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:23.188Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema).out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:23.224Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/_UnpickledSideInput(ParDo(UpdateDestinationSchema).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:23.389Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/_UnpickledSideInput(ParDo(UpdateDestinationSchema).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:23.474Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/_UnpickledSideInput(ParDo(UpdateDestinationSchema).out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:23.511Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/WaitForSchemaModJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:27.020Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/WaitForSchemaModJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:27.147Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:27.196Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(WaitForSchemaModJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:27.358Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(WaitForSchemaModJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:27.397Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(WaitForSchemaModJobs.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:27.482Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:31.672Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:31.710Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:31.746Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:31.914Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:31.950Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:31.989Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:35.719Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:35.754Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:35.792Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(WaitForCopyJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:35.962Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(WaitForCopyJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:36Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(WaitForCopyJobs.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:36.095Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:36.316Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:36.351Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:36.387Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:39.754Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:39.794Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:39.837Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:39.877Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Delete\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:40.243Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Delete\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:40.281Z: JOB_MESSAGE_DEBUG: Executing success step success50\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:40.339Z: JOB_MESSAGE_DETAILED: Cleaning up.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:40.378Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:49:40.485Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:01.428Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:01.506Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:01.526Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-30_00_28_55-10446954811771525401 is in state JOB_STATE_DONE\n","/usr/local/lib/python3.7/dist-packages/apache_beam/io/gcp/bigquery.py:2437: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n","  temp_location = pcoll.pipeline.options.view_as(\n","/usr/local/lib/python3.7/dist-packages/apache_beam/io/gcp/bigquery_file_loads.py:1128: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n","  temp_location = p.options.view_as(GoogleCloudOptions).temp_location\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/dataflow-requirements-cache', '-r', '/tmp/tmpnzl0ssog/tmp_requirements.txt', '--exists-action', 'i', '--no-binary', ':all:']\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmpssscu5j1']\n","warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n","\n","INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpssscu5j1', 'apache-beam==2.37.0', '--no-deps', '--no-binary', ':all:']\n","INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n","INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n","INFO:apache_beam.runners.portability.stager:Executing command: ['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/tmpssscu5j1', 'apache-beam==2.37.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n","INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl\n","WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n","INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.37.0\n","INFO:root:Using provided Python SDK container image: gcr.io/cloud-dataflow/v1beta3/python37:2.37.0\n","INFO:root:Python SDK container image set to \"gcr.io/cloud-dataflow/v1beta3/python37:2.37.0\" for Docker environment\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f300752fdd0> ====================\n","INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f30075315f0> ====================\n","INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n","INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n","INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n","INFO:oauth2client.client:Refreshing access_token\n","INFO:apache_beam.io.gcp.bigquery_tools:Started BigQuery job: <JobReference\n"," location: 'asia-northeast1'\n"," projectId: 'bwing-230309'>\n"," bq show -j --format=prettyjson --project_id=bwing-230309 None\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330075225-256443-sbocmw2d.1648626745.256695/requirements.txt...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330075225-256443-sbocmw2d.1648626745.256695/requirements.txt in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330075225-256443-sbocmw2d.1648626745.256695/dataflow_python_sdk.tar...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330075225-256443-sbocmw2d.1648626745.256695/dataflow_python_sdk.tar in 1 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330075225-256443-sbocmw2d.1648626745.256695/workflow.tar.gz...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330075225-256443-sbocmw2d.1648626745.256695/workflow.tar.gz in 0 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330075225-256443-sbocmw2d.1648626745.256695/apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330075225-256443-sbocmw2d.1648626745.256695/apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl in 5 seconds.\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330075225-256443-sbocmw2d.1648626745.256695/pipeline.pb...\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://bwing-230309_dataflow/code/beamapp-root-0330075225-256443-sbocmw2d.1648626745.256695/pipeline.pb in 0 seconds.\n","WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['dataflow_whatya_v1_kosfja.py']\n","WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['dataflow_whatya_v1_kosfja.py']\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n"," clientRequestId: '20220330075225257865-3705'\n"," createTime: '2022-03-30T07:52:37.481432Z'\n"," currentStateTime: '1970-01-01T00:00:00Z'\n"," id: '2022-03-30_00_52_36-8070475723528590929'\n"," location: 'asia-east1'\n"," name: 'beamapp-root-0330075225-256443-sbocmw2d'\n"," projectId: 'bwing-230309'\n"," stageStates: []\n"," startTime: '2022-03-30T07:52:37.481432Z'\n"," steps: []\n"," tempFiles: []\n"," type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2022-03-30_00_52_36-8070475723528590929]\n","INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2022-03-30_00_52_36-8070475723528590929\n","INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/asia-east1/2022-03-30_00_52_36-8070475723528590929?project=bwing-230309\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-30_00_52_36-8070475723528590929 is in state JOB_STATE_PENDING\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:37.618Z: JOB_MESSAGE_BASIC: The pipeline is using shuffle service with a (boot) persistent disk size / type other than the default. If that configuration was intended solely to speed up the non-service shuffle, consider removing it to reduce costs as those disks are unused by the shuffle service.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:38.020Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2022-03-30_00_52_36-8070475723528590929. The number of workers will be between 1 and 1.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:38.032Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2022-03-30_00_52_36-8070475723528590929.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:40.185Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-2 in asia-east1-c.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.528Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.546Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.563Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.581Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.611Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.628Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.697Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.733Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.753Z: JOB_MESSAGE_DETAILED: Unzipping flatten s25 for input s19.WrittenFiles\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.770Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround, through flatten WriteToBQ/BigQueryBatchFileLoads/DestinationFilesUnion, into producer WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.787Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.807Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow into WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.824Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.840Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs) into WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.857Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables into WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.875Z: JOB_MESSAGE_DETAILED: Unzipping flatten s25-u32 for input s26.None-c30\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.892Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify, through flatten WriteToBQ/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.908Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround into WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.924Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify into WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.940Z: JOB_MESSAGE_DETAILED: Fusing consumer ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough) into ReadFromBigQuery/ReadFromBigQuery/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:41.960Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze Sentiment into ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.002Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal into Analyze Sentiment\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.019Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/AppendDestination into WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.039Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into WriteToBQ/BigQueryBatchFileLoads/AppendDestination\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.056Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.071Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify into WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.088Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Write into WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.142Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow into WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.193Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/DropShardNumber into WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.209Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile into WriteToBQ/BigQueryBatchFileLoads/DropShardNumber\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.241Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.257Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.307Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.323Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix into WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.340Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/ParDo(UpdateDestinationSchema) into WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.358Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.375Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/WaitForSchemaModJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.392Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.409Z: JOB_MESSAGE_DETAILED: Fusing consumer ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove into ReadFromBigQuery/ReadFromBigQuery/FilesToRemoveImpulse/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.424Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.439Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.455Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.472Z: JOB_MESSAGE_DETAILED: Fusing consumer ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles) into ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/Create/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.489Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.527Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.543Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.559Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Delete into WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.577Z: JOB_MESSAGE_DETAILED: Fusing consumer WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs into WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.604Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.622Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.638Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.655Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.847Z: JOB_MESSAGE_DEBUG: Executing wait step start52\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.887Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/FilesToRemoveImpulse/Read+ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.909Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.920Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.927Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.934Z: JOB_MESSAGE_BASIC: Starting 1 workers in asia-east1-c...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-30_00_52_36-8070475723528590929 is in state JOB_STATE_RUNNING\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:42.945Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseEmptyPC/Read\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:52:43.006Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ImpulseEmptyPC/Read.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:53:30.907Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:53:48.867Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:23.589Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/FilesToRemoveImpulse/Read+ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:23.625Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/MapFilesToRemove.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:23.660Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:23.728Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:23.762Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.177Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseSingleElementPC/Read+WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix+WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.260Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/LoadJobNamePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.280Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/SchemaModJobNamePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.300Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/CopyJobNamePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.318Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/GenerateFilePrefix.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.333Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.351Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.368Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/_UnpickledSideInput(SchemaModJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.385Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.403Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.421Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.426Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.434Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.436Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/_UnpickledSideInput(SchemaModJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.455Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.465Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.470Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.484Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/_UnpickledSideInput(LoadJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.492Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.501Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/_UnpickledSideInput(SchemaModJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.520Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(CopyJobNamePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.537Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.554Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/_UnpickledSideInput(GenerateFilePrefix.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.595Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.956Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:24.996Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:25.032Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:25.194Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:25.310Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:57:25.347Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)+Analyze Sentiment+WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteToBQ/BigQueryBatchFileLoads/AppendDestination+WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:34.404Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)+Analyze Sentiment+WriteToBQ/BigQueryBatchFileLoads/RewindowIntoGlobal+WriteToBQ/BigQueryBatchFileLoads/AppendDestination+WriteToBQ/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+WriteToBQ/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:34.448Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough).cleanup_signal\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:34.463Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:34.486Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:34.509Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:34.546Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/DropShardNumber+WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:34.579Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:34.618Z: JOB_MESSAGE_DEBUG: Value \"ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:34.656Z: JOB_MESSAGE_BASIC: Executing operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/Create/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:37.024Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/Read+WriteToBQ/BigQueryBatchFileLoads/GroupShardedRows/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/DropShardNumber+WriteToBQ/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/WriteGroupedRecordsToFile+WriteToBQ/BigQueryBatchFileLoads/IdentityWorkaround+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Reify+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:37.059Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:37.103Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:37.169Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:37.841Z: JOB_MESSAGE_BASIC: Finished operation ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/Create/Read+ReadFromBigQuery/ReadFromBigQuery/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.453Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read+WriteToBQ/BigQueryBatchFileLoads/GroupFilesByTableDestinations/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/ParDo(TriggerLoadJobs)+WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/TriggerLoadJobsWithoutTempTables\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.490Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.508Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).TemporaryTables\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.523Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.539Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.560Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.576Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.592Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/Flatten\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.609Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.628Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.642Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/_UnpickledSideInput(ParDo(TriggerLoadJobs).out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.644Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.658Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(ParDo(TriggerLoadJobs).TemporaryTables.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.679Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/ParDo(UpdateDestinationSchema)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.701Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/Flatten\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.709Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/_UnpickledSideInput(TriggerLoadJobsWithoutTempTables.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.733Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/Flatten.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:43.748Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:44.786Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForDestinationLoadJobs/WaitForDestinationLoadJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:47.985Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs/WaitForTempTableLoadJobs+WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/ParDo(UpdateDestinationSchema)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:48.028Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForTempTableLoadJobs.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:48.043Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema).out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:48.082Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/_UnpickledSideInput(ParDo(UpdateDestinationSchema).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:48.146Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/_UnpickledSideInput(ParDo(UpdateDestinationSchema).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:48.258Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/_UnpickledSideInput(ParDo(UpdateDestinationSchema).out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:48.342Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/WaitForSchemaModJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:50.986Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs/WaitForSchemaModJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:51.028Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForSchemaModJobs.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:51.063Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(WaitForSchemaModJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:51.132Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(WaitForSchemaModJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:51.255Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/_UnpickledSideInput(WaitForSchemaModJobs.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:51.350Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:53.972Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/ParDo(TriggerCopyJobs)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:54.014Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:54.052Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:54.122Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:54.250Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/_UnpickledSideInput(ParDo(TriggerCopyJobs).out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:54.315Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:57.817Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Read+WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs/WaitForCopyJobs\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:57.856Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/WaitForCopyJobs.out\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:57.895Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(WaitForCopyJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:57.962Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(WaitForCopyJobs.out.0)\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:57.999Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/_UnpickledSideInput(WaitForCopyJobs.out.0).output\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:58.035Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:58.207Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Create\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:58.342Z: JOB_MESSAGE_DEBUG: Value \"WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Session\" materialized.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:58.383Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:59.580Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/PassTables/PassTables+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Reify+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:59.619Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:59.676Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Close\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T07:59:59.718Z: JOB_MESSAGE_BASIC: Executing operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Delete\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T08:00:00.058Z: JOB_MESSAGE_BASIC: Finished operation WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/GroupByWindow+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys+WriteToBQ/BigQueryBatchFileLoads/RemoveTempTables/Delete\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T08:00:00.100Z: JOB_MESSAGE_DEBUG: Executing success step success50\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T08:00:00.166Z: JOB_MESSAGE_DETAILED: Cleaning up.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T08:00:00.217Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T08:00:00.264Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T08:02:24.341Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T08:02:24.388Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n","INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-30T08:02:24.421Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n","INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-30_00_52_36-8070475723528590929 is in state JOB_STATE_DONE\n"]}]},{"cell_type":"markdown","source":["## Cloud Console"],"metadata":{"id":"b3silL-4eDi_"}},{"cell_type":"markdown","source":["https://console.cloud.google.com/dataflow/jobs?project=bwing-230309&cloudshell=false&hl=ja"],"metadata":{"id":"UX364KuNd-Eu"}}]}