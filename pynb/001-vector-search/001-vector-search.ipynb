{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"001-vector-search.ipynb","provenance":[{"file_id":"11WBCrwNzbNWN7QbMEwzy-8MZROOVQFnZ","timestamp":1623834556543},{"file_id":"https://github.com/kstathou/vector_engine/blob/master/notebooks/001_vector_search.ipynb","timestamp":1618044202026}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python [conda env:myenv]","language":"python","name":"conda-env-myenv-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5c0d412bfe5e497192bd03fe8fac504d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_37362730c2e146d3a363aa79fc0c5101","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3774cb7e03794ff0acbbe50a17672ceb","IPY_MODEL_a868605ba36a41bbaf710ea125dd75ab"]}},"37362730c2e146d3a363aa79fc0c5101":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3774cb7e03794ff0acbbe50a17672ceb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fb31204b348848ceb872b30311ddca19","_dom_classes":[],"description":"Batches: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":264,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":264,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e837d77bf5e4430fac05ddb58faf3562"}},"a868605ba36a41bbaf710ea125dd75ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a858b995fb8a4c279cc34c22a3623087","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 264/264 [00:31&lt;00:00,  8.50it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1e1a802ccd1745d5b22229c0e4fd7241"}},"fb31204b348848ceb872b30311ddca19":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e837d77bf5e4430fac05ddb58faf3562":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a858b995fb8a4c279cc34c22a3623087":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1e1a802ccd1745d5b22229c0e4fd7241":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"bJLQoimyVyQ8"},"source":["### Uncomment and run the following cells if you work on Google Colab :) Don't forget to change your runtime type to GPU!"]},{"cell_type":"code","metadata":{"id":"rVV81xc3VyQ9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623834623506,"user_tz":-540,"elapsed":2922,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"d8f2b222-b5ae-4c3c-e8b9-7ffd4e3e28b1"},"source":["!git clone https://github.com/kstathou/vector_engine"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'vector_engine'...\n","remote: Enumerating objects: 74, done.\u001b[K\n","remote: Counting objects: 100% (74/74), done.\u001b[K\n","remote: Compressing objects: 100% (53/53), done.\u001b[K\n","remote: Total 74 (delta 32), reused 59 (delta 18), pack-reused 0\u001b[K\n","Unpacking objects: 100% (74/74), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C0lSFLw3VyRG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623834623507,"user_tz":-540,"elapsed":8,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"2eb00396-d8c0-4d83-c548-b2f6021edc0b"},"source":["cd vector_engine"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/vector_engine\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5sOhWL6UVyRQ","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1623834658195,"user_tz":-540,"elapsed":34692,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"3f986a55-1244-4e96-e87c-2725a000d464"},"source":["pip install -r requirements.txt"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Obtaining file:///content/vector_engine (from -r requirements.txt (line 9))\n","Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.8.1+cu101)\n","Collecting transformers==3.3.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 31.3MB/s \n","\u001b[?25hCollecting sentence-transformers==0.3.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/fd/0190080aa0af78d7cd5874e4e8e85f0bed9967dd387cf05d760832b95da9/sentence-transformers-0.3.8.tar.gz (66kB)\n","\u001b[K     |████████████████████████████████| 71kB 11.4MB/s \n","\u001b[?25hCollecting pandas==1.1.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/69/18b96b520519818e00b04dd08d7cbc5e764f1465f5a280cf96173f34c54e/pandas-1.1.2-cp37-cp37m-manylinux1_x86_64.whl (10.5MB)\n","\u001b[K     |████████████████████████████████| 10.5MB 50.5MB/s \n","\u001b[?25hCollecting faiss-cpu==1.6.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/0b/c77bcf7c1053542ad60516258ff07c0e38e0792a6d27b927a5dd20926d13/faiss_cpu-1.6.1-cp37-cp37m-manylinux2010_x86_64.whl (7.1MB)\n","\u001b[K     |████████████████████████████████| 7.1MB 43.5MB/s \n","\u001b[?25hCollecting numpy==1.19.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/04/c3846024ddc7514cde17087f62f0502abf85c53e8f69f6312c70db6d144e/numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5MB)\n","\u001b[K     |████████████████████████████████| 14.5MB 212kB/s \n","\u001b[?25hCollecting folium==0.2.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/dd/75ced7437bfa7cb9a88b96ee0177953062803c3b4cde411a97d98c35adaf/folium-0.2.1.tar.gz (69kB)\n","\u001b[K     |████████████████████████████████| 71kB 11.4MB/s \n","\u001b[?25hCollecting streamlit==0.62.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/68/33610a2a4189714e4a37ea1e4ebf38cb05bff251b3863c37a5887532872c/streamlit-0.62.0-py2.py3-none-any.whl (7.1MB)\n","\u001b[K     |████████████████████████████████| 7.1MB 24.2MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->-r requirements.txt (line 1)) (3.7.4.3)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 52.8MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 55.2MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r requirements.txt (line 2)) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r requirements.txt (line 2)) (20.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r requirements.txt (line 2)) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r requirements.txt (line 2)) (4.41.1)\n","Collecting tokenizers==0.8.1.rc2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/26/c02ba92ecb8b780bdae4a862d351433c2912fe49469dac7f87a5c85ccca6/tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 54.6MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1->-r requirements.txt (line 2)) (2019.12.20)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.8->-r requirements.txt (line 3)) (0.22.2.post1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.8->-r requirements.txt (line 3)) (1.4.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.8->-r requirements.txt (line 3)) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.2->-r requirements.txt (line 4)) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.2->-r requirements.txt (line 4)) (2018.9)\n","Requirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1->-r requirements.txt (line 7)) (2.11.3)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.62.0->-r requirements.txt (line 8)) (7.1.2)\n","Collecting watchdog\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/5b/36b3b11e557830de6fc1dc06e9aa3ee274119b8cea9cc98175dbbf72cf87/watchdog-2.1.2-py3-none-manylinux2014_x86_64.whl (74kB)\n","\u001b[K     |████████████████████████████████| 81kB 12.4MB/s \n","\u001b[?25hCollecting base58\n","  Downloading https://files.pythonhosted.org/packages/b8/a1/d9f565e9910c09fd325dc638765e8843a19fa696275c16cc08cf3b0a3c25/base58-2.1.0-py3-none-any.whl\n","Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.62.0->-r requirements.txt (line 8)) (5.1.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.62.0->-r requirements.txt (line 8)) (7.1.2)\n","Collecting validators\n","  Downloading https://files.pythonhosted.org/packages/db/2f/7fed3ee94ad665ad2c1de87f858f10a7785251ff75b4fd47987888d07ef1/validators-0.18.2-py3-none-any.whl\n","Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit==0.62.0->-r requirements.txt (line 8)) (1.5.1)\n","Collecting blinker\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/51/e2a9f3b757eb802f61dc1f2b09c8c99f6eb01cf06416c0671253536517b6/blinker-1.4.tar.gz (111kB)\n","\u001b[K     |████████████████████████████████| 112kB 60.5MB/s \n","\u001b[?25hCollecting enum-compat\n","  Downloading https://files.pythonhosted.org/packages/55/ae/467bc4509246283bb59746e21a1a2f5a8aecbef56b1fa6eaca78cd438c8b/enum_compat-0.0.3-py3-none-any.whl\n","Collecting boto3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/55/e66b557bdbc266ab4f15249f382f5d7d165fee1caa7e12c96348c05ea53d/boto3-1.17.95-py2.py3-none-any.whl (131kB)\n","\u001b[K     |████████████████████████████████| 133kB 57.1MB/s \n","\u001b[?25hRequirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit==0.62.0->-r requirements.txt (line 8)) (0.8.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.62.0->-r requirements.txt (line 8)) (3.12.4)\n","Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.62.0->-r requirements.txt (line 8)) (4.1.0)\n","Collecting pydeck>=0.1.dev5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/bc/f0e44828e4290367c869591d50d3671a4d0ee94926da6cb734b7b200308c/pydeck-0.6.2-py2.py3-none-any.whl (4.2MB)\n","\u001b[K     |████████████████████████████████| 4.2MB 54.1MB/s \n","\u001b[?25hRequirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.62.0->-r requirements.txt (line 8)) (4.2.2)\n","Collecting botocore>=1.13.44\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/70/43ebe47ab7591eca35be1dcbb0e4ede2a3cf15915ad05876920b63296988/botocore-1.20.95-py2.py3-none-any.whl (7.6MB)\n","\u001b[K     |████████████████████████████████| 7.6MB 37.4MB/s \n","\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit==0.62.0->-r requirements.txt (line 8)) (0.10.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1->-r requirements.txt (line 2)) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1->-r requirements.txt (line 2)) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.1->-r requirements.txt (line 2)) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r requirements.txt (line 2)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r requirements.txt (line 2)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r requirements.txt (line 2)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1->-r requirements.txt (line 2)) (2021.5.30)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1->-r requirements.txt (line 7)) (2.0.1)\n","Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit==0.62.0->-r requirements.txt (line 8)) (4.4.2)\n","Collecting s3transfer<0.5.0,>=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n","\u001b[K     |████████████████████████████████| 81kB 13.7MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.0->streamlit==0.62.0->-r requirements.txt (line 8)) (57.0.0)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.62.0->-r requirements.txt (line 8)) (2.6.0)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.62.0->-r requirements.txt (line 8)) (0.11.1)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.62.0->-r requirements.txt (line 8)) (0.3)\n","Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (5.0.5)\n","Collecting ipykernel>=5.1.2; python_version >= \"3.4\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/6d/6c8fe4b658f77947d4244ce81f60230c4c8d1dc1a21ae83e63b269339178/ipykernel-5.5.5-py3-none-any.whl (120kB)\n","\u001b[K     |████████████████████████████████| 122kB 59.7MB/s \n","\u001b[?25hRequirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (7.6.3)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (0.2.0)\n","Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (5.5.0)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (5.3.5)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (1.0.0)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (3.5.1)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (5.1.3)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (2.6.1)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (4.8.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (1.0.18)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (0.8.1)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (4.7.1)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (22.1.0)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (5.3.1)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (0.2.5)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (5.6.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (1.5.0)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (0.10.1)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (0.7.1)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (3.3.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (1.4.3)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (0.5.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (0.8.4)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.62.0->-r requirements.txt (line 8)) (0.5.1)\n","Building wheels for collected packages: sentence-transformers, folium, blinker\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.8-cp37-none-any.whl size=101996 sha256=fe27605e2a47e74051674694d31ea32602465bcc5958460bd9d5bc7ecb21dda6\n","  Stored in directory: /root/.cache/pip/wheels/27/ec/b3/d12cc8e4daf77846db6543033d3a5642f204c0320b15945647\n","  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for folium: filename=folium-0.2.1-cp37-none-any.whl size=79810 sha256=30a13eb72893f9b43b8ba2fae626d3a7b06b03e400815326863672c860efb0f3\n","  Stored in directory: /root/.cache/pip/wheels/b8/09/f0/52d2ef419c2aaf4fb149f92a33e0008bdce7ae816f0dd8f0c5\n","  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for blinker: filename=blinker-1.4-cp37-none-any.whl size=13476 sha256=190e3f2bfd9c863fe252ce735b3e827823ee7e1b64cbbba2ec940bddbbb1d33d\n","  Stored in directory: /root/.cache/pip/wheels/92/a0/00/8690a57883956a301d91cf4ec999cc0b258b01e3f548f86e89\n","Successfully built sentence-transformers folium blinker\n","\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.5.5 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","\u001b[31mERROR: botocore 1.20.95 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n","Installing collected packages: numpy, sentencepiece, sacremoses, tokenizers, transformers, sentence-transformers, pandas, faiss-cpu, folium, watchdog, base58, validators, blinker, enum-compat, jmespath, botocore, s3transfer, boto3, ipykernel, pydeck, streamlit, vector-engine\n","  Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Found existing installation: pandas 1.1.5\n","    Uninstalling pandas-1.1.5:\n","      Successfully uninstalled pandas-1.1.5\n","  Found existing installation: folium 0.8.3\n","    Uninstalling folium-0.8.3:\n","      Successfully uninstalled folium-0.8.3\n","  Found existing installation: ipykernel 4.10.1\n","    Uninstalling ipykernel-4.10.1:\n","      Successfully uninstalled ipykernel-4.10.1\n","  Running setup.py develop for vector-engine\n","Successfully installed base58-2.1.0 blinker-1.4 boto3-1.17.95 botocore-1.20.95 enum-compat-0.0.3 faiss-cpu-1.6.1 folium-0.2.1 ipykernel-5.5.5 jmespath-0.10.0 numpy-1.19.2 pandas-1.1.2 pydeck-0.6.2 s3transfer-0.4.2 sacremoses-0.0.45 sentence-transformers-0.3.8 sentencepiece-0.1.95 streamlit-0.62.0 tokenizers-0.8.1rc2 transformers-3.3.1 validators-0.18.2 vector-engine watchdog-2.1.2\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["ipykernel","numpy","pandas"]}}},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"vbnscDwgVyRW"},"source":["### Let's begin!"]},{"cell_type":"code","metadata":{"id":"v7ftrzzmVyRX","executionInfo":{"status":"ok","timestamp":1623834658196,"user_tz":-540,"elapsed":7,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}}},"source":["%load_ext autoreload"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"fU2i4vlCVyRc","executionInfo":{"status":"ok","timestamp":1623834664439,"user_tz":-540,"elapsed":6248,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}}},"source":["%autoreload 2\n","# Used to import data from local.\n","import pandas as pd\n","\n","# Used to create the dense document vectors.\n","import torch\n","from sentence_transformers import SentenceTransformer\n","\n","# Used to create and store the Faiss index.\n","import faiss\n","import numpy as np\n","import pickle\n","from pathlib import Path\n","\n","# Used to do vector searches and display the results.\n","from vector_engine.utils import vector_search, id2details"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kz5YBwU5VyRi"},"source":["Stored and processed data in s3"]},{"cell_type":"code","metadata":{"id":"VEANywYAVyRi","executionInfo":{"status":"ok","timestamp":1623834664444,"user_tz":-540,"elapsed":9,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}}},"source":["# Read a CSV in a table\n","df = pd.read_csv('data/misinformation_papers.csv')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":133},"id":"HJXljSbYVyRn","executionInfo":{"status":"ok","timestamp":1623834664788,"user_tz":-540,"elapsed":351,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"b9162fc7-f5f1-424a-b91c-c79c59e34f48"},"source":["df.head(3)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>original_title</th>\n","      <th>abstract</th>\n","      <th>year</th>\n","      <th>citations</th>\n","      <th>id</th>\n","      <th>is_EN</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>When Corrections Fail: The Persistence of Poli...</td>\n","      <td>An extensive literature addresses citizen igno...</td>\n","      <td>2010</td>\n","      <td>901</td>\n","      <td>2132553681</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A postmodern Pandora's box: anti-vaccination m...</td>\n","      <td>The Internet plays a large role in disseminati...</td>\n","      <td>2010</td>\n","      <td>440</td>\n","      <td>2117485795</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Spread of (Mis)Information in Social Networks</td>\n","      <td>We provide a model to investigate the tension ...</td>\n","      <td>2010</td>\n","      <td>278</td>\n","      <td>2120015072</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                      original_title  ... is_EN\n","0  When Corrections Fail: The Persistence of Poli...  ...     1\n","1  A postmodern Pandora's box: anti-vaccination m...  ...     1\n","2      Spread of (Mis)Information in Social Networks  ...     1\n","\n","[3 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MljadlGpVyRs","executionInfo":{"status":"ok","timestamp":1623834664789,"user_tz":-540,"elapsed":5,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"35c2c2a0-c8d2-4cf4-cff5-f09114ffe2f4"},"source":["print(f\"Misinformation, disinformation and fake news papers: {df.id.unique().shape[0]}\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Misinformation, disinformation and fake news papers: 8430\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VyRG1wZLVyRw"},"source":["The [Sentence Transformers library](https://github.com/UKPLab/sentence-transformers) offers pretrained transformers that produce SOTA sentence embeddings. Checkout this [spreadsheet](https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/) with all the available models.\n","\n","In this tutorial, we will use the `distilbert-base-nli-stsb-mean-tokens` model which has the best performance on Semantic Textual Similarity tasks among the DistilBERT versions. Moreover, although it's slightly worse than BERT, it is quite faster thanks to having a smaller size.\n","\n","I use the same model in [Orion's semantic search engine](https://www.orion-search.org/)!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PjF6CrwUVyRx","executionInfo":{"status":"ok","timestamp":1623834692270,"user_tz":-540,"elapsed":27484,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"26157845-3c67-4b9d-953a-79f8c8226245"},"source":["# Instantiate the sentence-level DistilBERT\n","model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n","# Check if GPU is available and use it\n","if torch.cuda.is_available():\n","    model = model.to(torch.device(\"cuda\"))\n","print(model.device)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["100%|██████████| 245M/245M [00:12<00:00, 19.7MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["5c0d412bfe5e497192bd03fe8fac504d","37362730c2e146d3a363aa79fc0c5101","3774cb7e03794ff0acbbe50a17672ceb","a868605ba36a41bbaf710ea125dd75ab","fb31204b348848ceb872b30311ddca19","e837d77bf5e4430fac05ddb58faf3562","a858b995fb8a4c279cc34c22a3623087","1e1a802ccd1745d5b22229c0e4fd7241"]},"id":"Y_GS0_CWVyR1","executionInfo":{"status":"ok","timestamp":1623834724080,"user_tz":-540,"elapsed":31821,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"0e53db7d-c759-426a-c701-069e4c90a227"},"source":["# Convert abstracts to vectors\n","embeddings = model.encode(df.abstract.to_list(), show_progress_bar=True)"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c0d412bfe5e497192bd03fe8fac504d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Batches', max=264.0, style=ProgressStyle(description_widt…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gE7w-RJbVyR6","executionInfo":{"status":"ok","timestamp":1623834724081,"user_tz":-540,"elapsed":8,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"f40c4f70-bee9-417b-b628-ed0d631de956"},"source":["print(f'Shape of the vectorised abstract: {embeddings[0].shape}')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Shape of the vectorised abstract: (768,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YGV4Je1EVyR_"},"source":["## Vector similarity search with Faiss\n","[Faiss](https://github.com/facebookresearch/faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, even ones that do not fit in RAM. \n","    \n","Faiss is built around the `Index` object which contains, and sometimes preprocesses, the searchable vectors. Faiss has a large collection of [indexes](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes). You can even create [composite indexes](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes-(composite)). Faiss handles collections of vectors of a fixed dimensionality d, typically a few 10s to 100s.\n","\n","**Note**: Faiss uses only 32-bit floating point matrices. This means that you will have to change the data type of the input before building the index.\n","\n","To learn more about Faiss, you can read their paper on [arXiv](https://arxiv.org/abs/1702.08734).\n","\n","Here, we will the `IndexFlatL2` index:\n","- It's a simple index that performs a brute-force L2 distance search\n","- It scales linearly. It will work fine with our data but you might want to try [faster indexes](https://github.com/facebookresearch/faiss/wiki/Faster-search) if you work will millions of vectors.\n","\n","To create an index with the `misinformation` abstract vectors, we will:\n","1. Change the data type of the abstract vectors to float32.\n","2. Build an index and pass it the dimension of the vectors it will operate on.\n","3. Pass the index to IndexIDMap, an object that enables us to provide a custom list of IDs for the indexed vectors.\n","4. Add the abstract vectors and their ID mapping to the index. In our case, we will map vectors to their paper IDs from MAG."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8kkUDtwHVyR_","executionInfo":{"status":"ok","timestamp":1623834724081,"user_tz":-540,"elapsed":5,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"677f3944-2851-4fec-f702-487c2ae35058"},"source":["# Step 1: Change data type\n","embeddings = np.array([embedding for embedding in embeddings]).astype(\"float32\")\n","\n","# Step 2: Instantiate the index\n","index = faiss.IndexFlatL2(embeddings.shape[1])\n","\n","# Step 3: Pass the index to IndexIDMap\n","index = faiss.IndexIDMap(index)\n","\n","# Step 4: Add vectors and their IDs\n","index.add_with_ids(embeddings, df.id.values)\n","\n","print(f\"Number of vectors in the Faiss index: {index.ntotal}\")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Number of vectors in the Faiss index: 8430\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yt1z-433VySE"},"source":["### Searching the index\n","The index we built will perform a k-nearest-neighbour search. We have to provide the number of neighbours to be returned. \n","\n","Let's query the index with an abstract from our dataset and retrieve the 10 most relevant documents. **The first one must be our query!**\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"eEeJt7lYVySN","executionInfo":{"status":"ok","timestamp":1623834724594,"user_tz":-540,"elapsed":516,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"357bfecb-2ae5-4ec2-b35f-38209ae435ba"},"source":["# Paper abstract\n","df.iloc[5415, 1]"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"We address the diffusion of information about the COVID-19 with a massive data analysis on Twitter, Instagram, YouTube, Reddit and Gab. We analyze engagement and interest in the COVID-19 topic and provide a differential assessment on the evolution of the discourse on a global scale for each platform and their users. We fit information spreading with epidemic models characterizing the basic reproduction number [Formula: see text] for each social media platform. Moreover, we identify information spreading from questionable sources, finding different volumes of misinformation in each platform. However, information from both reliable and questionable sources do not present different spreading patterns. Finally, we provide platform-dependent numerical estimates of rumors' amplification.\""]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BSuRcH85VySQ","executionInfo":{"status":"ok","timestamp":1623834724594,"user_tz":-540,"elapsed":21,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"353745bc-af7b-431e-d3e5-a53e30901885"},"source":["# Retrieve the 10 nearest neighbours\n","D, I = index.search(np.array([embeddings[5415]]), k=10)\n","print(f'L2 distance: {D.flatten().tolist()}\\n\\nMAG paper IDs: {I.flatten().tolist()}')"],"execution_count":14,"outputs":[{"output_type":"stream","text":["L2 distance: [0.0, 1.267284631729126, 62.72160339355469, 63.670326232910156, 64.58393859863281, 67.47344970703125, 67.96402740478516, 69.47564697265625, 72.5633544921875, 74.62230682373047]\n","\n","MAG paper IDs: [3092618151, 3011345566, 3012936764, 3055557295, 3011186656, 3044429417, 3092128270, 3024620668, 3047284882, 3048848247]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SiO1pa4oVySU","executionInfo":{"status":"ok","timestamp":1623834724595,"user_tz":-540,"elapsed":19,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"ce64eb66-f6cb-4878-d092-c2220d7a9310"},"source":["# Fetch the paper titles based on their index\n","id2details(df, I, 'original_title')"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['The COVID-19 social media infodemic.'],\n"," ['The COVID-19 Social Media Infodemic'],\n"," ['Understanding the perception of COVID-19 policies by mining a multilanguage Twitter dataset'],\n"," ['Covid-19 infodemic reveals new tipping point epidemiology and a revised R formula.'],\n"," ['Coronavirus Goes Viral: Quantifying the COVID-19 Misinformation Epidemic on Twitter'],\n"," ['Effects of misinformation on COVID-19 individual responses and recommendations for resilience of disastrous consequences of misinformation'],\n"," ['Analysis of online misinformation during the peak of the COVID-19 pandemics in Italy'],\n"," ['Quantifying COVID-19 Content in the Online Health Opinion War Using Machine Learning'],\n"," ['Global Infodemiology of COVID-19: Analysis of Google Web Searches and Instagram Hashtags.'],\n"," ['COVID-19-Related Infodemic and Its Impact on Public Health: A Global Social Media Analysis.']]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p29pEtGrWUMV","executionInfo":{"status":"ok","timestamp":1623834724596,"user_tz":-540,"elapsed":16,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"e0adbf85-8e83-4721-b1f3-8512cd241402"},"source":["# Fetch the paper abstracts based on their index\n","id2details(df, I, 'abstract')"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[\"We address the diffusion of information about the COVID-19 with a massive data analysis on Twitter, Instagram, YouTube, Reddit and Gab. We analyze engagement and interest in the COVID-19 topic and provide a differential assessment on the evolution of the discourse on a global scale for each platform and their users. We fit information spreading with epidemic models characterizing the basic reproduction number [Formula: see text] for each social media platform. Moreover, we identify information spreading from questionable sources, finding different volumes of misinformation in each platform. However, information from both reliable and questionable sources do not present different spreading patterns. Finally, we provide platform-dependent numerical estimates of rumors' amplification.\"],\n"," [\"We address the diffusion of information about the COVID-19 with a massive data analysis on Twitter, Instagram, YouTube, Reddit and Gab. We analyze engagement and interest in the COVID-19 topic and provide a differential assessment on the evolution of the discourse on a global scale for each platform and their users. We fit information spreading with epidemic models characterizing the basic reproduction numbers $R_0$ for each social media platform. Moreover, we characterize information spreading from questionable sources, finding different volumes of misinformation in each platform. However, information from both reliable and questionable sources do not present different spreading patterns. Finally, we provide platform-dependent numerical estimates of rumors' amplification.\"],\n"," ['The objective of this work is to explore popular discourse about the COVID-19 pandemic and policies implemented to manage it. Using Natural Language Processing, Text Mining, and Network Analysis to analyze corpus of tweets that relate to the COVID-19 pandemic, we identify common responses to the pandemic and how these responses differ across time. Moreover, insights as to how information and misinformation were transmitted via Twitter, starting at the early stages of this pandemic, are presented. Finally, this work introduces a dataset of tweets collected from all over the world, in multiple languages, dating back to January 22nd, when the total cases of reported COVID-19 were below 600 worldwide. The insights presented in this work could help inform decision makers in the face of future pandemics, and the dataset introduced can be used to acquire valuable knowledge to help mitigate the COVID-19 pandemic.'],\n"," [\"Many governments have managed to control their COVID-19 outbreak with a simple message: keep the effective '$R$ number' $R<1$ to prevent widespread contagion and flatten the curve. This raises the question whether a similar policy could control dangerous online 'infodemics' of information, misinformation and disinformation. Here we show, using multi-platform data from the COVID-19 infodemic, that its online spreading instead encompasses a different dynamical regime where communities and users within and across independent platforms, sporadically form temporary active links on similar timescales to the viral spreading. This allows material that might have died out, to evolve and even mutate. This has enabled niche networks that were already successfully spreading hate and anti-vaccination material, to rapidly become global super-spreaders of narratives featuring fake COVID-19 treatments, anti-Asian sentiment and conspiracy theories. We derive new tools that incorporate these coupled social-viral dynamics, including an online $R$, to help prevent infodemic spreading at all scales: from spreading across platforms (e.g. Facebook, 4Chan) to spreading within a given subpopulation, or community, or topic. By accounting for similar social and viral timescales, the same mathematical theory also offers a quantitative description of other unconventional infection profiles such as rumors spreading in financial markets and colds spreading in schools.\"],\n"," ['Background Since the beginning of the coronavirus disease 2019 (COVID-19) epidemic, misinformation has been spreading\\xa0uninhibited\\xa0over traditional and social media at a rapid pace. We sought to analyze the magnitude of misinformation that is being spread on Twitter\\xa0(Twitter, Inc., San Francisco, CA) regarding the coronavirus epidemic.\\xa0 Materials and methods We conducted a search on Twitter using 14 different trending hashtags and keywords related to the COVID-19 epidemic. We then summarized and assessed individual tweets for misinformation in comparison to verified and peer-reviewed resources. Descriptive statistics were used to compare\\xa0terms and hashtags, and to identify individual tweets and account characteristics. Results The study included 673 tweets. Most tweets were posted by informal individuals/groups (66%), and 129 (19.2%) belonged to verified Twitter accounts. The majority of included tweets contained serious content (91.2%); 548 tweets (81.4%) included genuine information pertaining to the COVID-19 epidemic. Around 70% of the tweets tackled medical/public health information, while the others were pertaining to sociopolitical and financial factors. In total, 153 tweets (24.8%) included misinformation, and 107 (17.4%) included unverifiable information regarding the COVID-19 epidemic. The rate of misinformation was higher among informal individual/group accounts (33.8%, p: <0.001). Tweets from unverified Twitter accounts contained more misinformation (31.0% vs 12.6% for verified accounts, p: <0.001). Tweets from healthcare/public health accounts had the lowest rate of unverifiable information (12.3%, p: 0.04). The number of likes and retweets per tweet was not associated with a difference in either false or unverifiable content. The keyword \"COVID-19\" had the lowest rate of misinformation and unverifiable information, while the keywords \"#2019_ncov\" and \"Corona\" were associated with the highest amount of misinformation and unverifiable content respectively. Conclusions Medical misinformation and unverifiable content pertaining to the global COVID-19 epidemic are being propagated at an alarming rate on social media. We provide an early quantification of the magnitude of misinformation spread and highlight the importance of early interventions in order to curb this phenomenon that endangers public safety at a time when awareness and appropriate preventive actions are paramount.'],\n"," ['Abstract The proliferation of misinformation on social media platforms is faster than the spread of Corona Virus Diseases (COVID-19) and it can generate hefty deleterious consequences on health amid a disaster like COVID-19. Drawing upon research on the stimulus-response theory (hypodermic needle theory) and the resilience theory, this study tested a conceptual framework considering general misinformation belief, conspiracy belief, and religious misinformation belief as the stimulus; and credibility evaluations as resilience strategy; and their effects on COVID-19 individual responses. Using a self-administered online survey during the COVID-19 pandemic, the study obtained 483 useable responses and after test, finds that all-inclusive, the propagation of misinformation on social media undermines the COVID-19 individual responses. Particularly, credibility evaluation of misinformation strongly predicts the COVID-19 individual responses with positive influences and religious misinformation beliefs as well as conspiracy beliefs and general misinformation beliefs come next and influence negatively. The findings and general recommendations will help the public, in general, to be cautious about misinformation, and the respective authority of a country, in particular, for initiating proper safety measures about disastrous misinformation to protect the public health from being exploited.'],\n"," ['During the Covid-19 pandemics, we also experience another dangerous pandemics based on misinformation. Narratives disconnected from fact-checking on the origin and cure of the disease intertwined with pre-existing political fights. We collect a database on Twitter posts and analyse the topology of the networks of retweeters (users broadcasting again the same elementary piece of information, or tweet) and validate its structure with methods of statistical physics of networks. Furthermore, by using commonly available fact checking software, we assess the reputation of the pieces of news exchanged. By using a combination of theoretical and practical weapons, we are able to track down the flow of misinformation in a snapshot of the Twitter ecosystem. Thanks to the presence of verified users, we can also assign a polarization to the network nodes (users) and see the impact of low-quality information producers and spreaders in the Twitter ecosystem.'],\n"," ['A huge amount of potentially dangerous COVID-19 misinformation is appearing online. Here we use machine learning to quantify COVID-19 content among online opponents of establishment health guidance, in particular vaccinations (“anti-vax”). We find that the anti-vax community is developing a less focused debate around COVID-19 than its counterpart, the pro-vaccination (“pro-vax”) community. However, the anti-vax community exhibits a broader range of “flavors” of COVID-19 topics, and hence can appeal to a broader cross-section of individuals seeking COVID-19 guidance online, e.g. individuals wary of a mandatory fast-tracked COVID-19 vaccine or those seeking alternative remedies. Hence the anti-vax community looks better positioned to attract fresh support going forward than the pro-vax community. This is concerning since a widespread lack of adoption of a COVID-19 vaccine will mean the world falls short of providing herd immunity, leaving countries open to future COVID-19 resurgences. We provide a mechanistic model that interprets these results and could help in assessing the likely efficacy of intervention strategies. Our approach is scalable and hence tackles the urgent problem facing social media platforms of having to analyze huge volumes of online health misinformation and disinformation.'],\n"," ['BACKGROUND: Although \"infodemiological\" methods have been used in research on coronavirus disease (COVID-19), an examination of the extent of infodemic moniker (misinformation) use on the internet remains limited. OBJECTIVE: The aim of this paper is to investigate internet search behaviors related to COVID-19 and examine the circulation of infodemic monikers through two platforms-Google and Instagram-during the current global pandemic. METHODS: We have defined infodemic moniker as a term, query, hashtag, or phrase that generates or feeds fake news, misinterpretations, or discriminatory phenomena. Using Google Trends and Instagram hashtags, we explored internet search activities and behaviors related to the COVID-19 pandemic from February 20, 2020, to May 6, 2020. We investigated the names used to identify the virus, health and risk perception, life during the lockdown, and information related to the adoption of COVID-19 infodemic monikers. We computed the average peak volume with a 95% CI for the monikers. RESULTS: The top six COVID-19-related terms searched in Google were \"coronavirus,\" \"corona,\" \"COVID,\" \"virus,\" \"corona virus,\" and \"COVID-19.\" Countries with a higher number of COVID-19 cases had a higher number of COVID-19 queries on Google. The monikers \"coronavirus ozone,\" \"coronavirus laboratory,\" \"coronavirus 5G,\" \"coronavirus conspiracy,\" and \"coronavirus bill gates\" were widely circulated on the internet. Searches on \"tips and cures\" for COVID-19 spiked in relation to the US president speculating about a \"miracle cure\" and suggesting an injection of disinfectant to treat the virus. Around two thirds (n=48,700,000, 66.1%) of Instagram users used the hashtags \"COVID-19\" and \"coronavirus\" to disperse virus-related information. CONCLUSIONS: Globally, there is a growing interest in COVID-19, and numerous infodemic monikers continue to circulate on the internet. Based on our findings, we hope to encourage mass media regulators and health organizers to be vigilant and diminish the use and circulation of these infodemic monikers to decrease the spread of misinformation.'],\n"," ['Infodemics, often including rumors, stigma, and conspiracy theories, have been common during the COVID-19 pandemic. Monitoring social media data has been identified as the best method for tracking rumors in real time and as a possible way to dispel misinformation and reduce stigma. However, the detection, assessment, and response to rumors, stigma, and conspiracy theories in real time are a challenge. Therefore, we followed and examined COVID-19-related rumors, stigma, and conspiracy theories circulating on online platforms, including fact-checking agency websites, Facebook, Twitter, and online newspapers, and their impacts on public health. Information was extracted between December 31, 2019 and April 5, 2020, and descriptively analyzed. We performed a content analysis of the news articles to compare and contrast data collected from other sources. We identified 2,311 reports of rumors, stigma, and conspiracy theories in 25 languages from 87 countries. Claims were related to illness, transmission and mortality (24%), control measures (21%), treatment and cure (19%), cause of disease including the origin (15%), violence (1%), and miscellaneous (20%). Of the 2,276 reports for which text ratings were available, 1,856 claims were false (82%). Misinformation fueled by rumors, stigma, and conspiracy theories can have potentially serious implications on the individual and community if prioritized over evidence-based guidelines. Health agencies must track misinformation associated with the COVID-19 in real time, and engage local communities and government stakeholders to debunk misinformation.']]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"gFKvRb4QY-DL"},"source":["\n","## Putting all together\n","\n","So far, we've built a Faiss index using the misinformation abstract vectors we encoded with a sentence-DistilBERT model. That's helpful but in a real case scenario, we would have to work with unseen data. To query the index with an unseen query and retrieve its most relevant documents, we would have to do the following:\n","\n","1. Encode the query with the same sentence-DistilBERT model we used for the rest of the abstract vectors.\n","2. Change its data type to float32.\n","3. Search the index with the encoded query.\n","\n","Here, we will use the introduction of an article published on [HKS Misinformation Review](https://misinforeview.hks.harvard.edu/article/can-whatsapp-benefit-from-debunked-fact-checked-stories-to-reduce-misinformation/).\n"]},{"cell_type":"code","metadata":{"id":"iDhftkrhX99T","executionInfo":{"status":"ok","timestamp":1623834724596,"user_tz":-540,"elapsed":14,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}}},"source":["user_query = \"\"\"\n","WhatsApp was alleged to have been widely used to spread misinformation and propaganda \n","during the 2018 elections in Brazil and the 2019 elections in India. Due to the \n","private encrypted nature of the messages on WhatsApp, it is hard to track the dissemination \n","of misinformation at scale. In this work, using public WhatsApp data from Brazil and India, we \n","observe that misinformation has been largely shared on WhatsApp public groups even after they \n","were already fact-checked by popular fact-checking agencies. This represents a significant portion \n","of misinformation spread in both Brazil and India in the groups analyzed. We posit that such \n","misinformation content could be prevented if WhatsApp had a means to flag already fact-checked \n","content. To this end, we propose an architecture that could be implemented by WhatsApp to counter \n","such misinformation. Our proposal respects the current end-to-end encryption architecture on WhatsApp, \n","thus protecting users’ privacy while providing an approach to detect the misinformation that benefits \n","from fact-checking efforts.\n","\"\"\""],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6AFhbGnWZpWN","executionInfo":{"status":"ok","timestamp":1623834724597,"user_tz":-540,"elapsed":15,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"3e080b71-cd7e-47e8-9565-85ef4cd30bd3"},"source":["# For convenience, I've wrapped all steps in the vector_search function.\n","# It takes four arguments: \n","# A query, the sentence-level transformer, the Faiss index and the number of requested results\n","D, I = vector_search([user_query], model, index, num_results=10)\n","print(f'L2 distance: {D.flatten().tolist()}\\n\\nMAG paper IDs: {I.flatten().tolist()}')"],"execution_count":18,"outputs":[{"output_type":"stream","text":["L2 distance: [7.384466171264648, 57.3224983215332, 57.3224983215332, 71.48453521728516, 72.06803131103516, 79.13472747802734, 86.0128173828125, 89.91024780273438, 90.76014709472656, 90.76422119140625]\n","\n","MAG paper IDs: [3047438096, 3021927925, 3037966274, 2889959140, 2791045616, 2943077655, 3014380170, 2967434249, 3028584171, 2990343632]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tbanjBhBZtWZ","executionInfo":{"status":"ok","timestamp":1623834725029,"user_tz":-540,"elapsed":442,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"cb83462b-d56c-4625-b05d-9323176e9342"},"source":["# Fetching the paper titles based on their index\n","id2details(df, I, 'original_title')"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['Can WhatsApp Benefit from Debunked Fact-Checked Stories to Reduce Misinformation?'],\n"," ['A Dataset of Fact-Checked Images Shared on WhatsApp During the Brazilian and Indian Elections'],\n"," ['A Dataset of Fact-Checked Images Shared on WhatsApp During the Brazilian and Indian Elections'],\n"," ['A System for Monitoring Public Political Groups in WhatsApp'],\n"," ['Politics of Fake News: How WhatsApp Became a Potent Propaganda Tool in India'],\n"," ['Characterizing Attention Cascades in WhatsApp Groups'],\n"," ['OS IMPACTOS JURÍDICOS E SOCIAIS DAS FAKE NEWS EM TERRITÓRIO BRASILEIRO'],\n"," ['Fake News and Social Media: Indian Perspective'],\n"," ['Images and Misinformation in Political Groups: Evidence from WhatsApp in India'],\n"," ['Can WhatsApp Counter Misinformation by Limiting Message Forwarding']]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rbxFKF-DZxg0","executionInfo":{"status":"ok","timestamp":1623834762784,"user_tz":-540,"elapsed":217,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}},"outputId":"093448b0-3409-442e-d0f5-84b9a5a64bf8"},"source":["# Define project base directory\n","# Change the index from 1 to 0 if you run this on Google Colab\n","# project_dir = Path('notebooks').resolve().parents[1]\n","project_dir = Path('notebooks').resolve().parents[0]\n","print(project_dir)\n","\n","# Serialise index and store it as a pickle\n","with open(f\"{project_dir}/models/faiss_index.pickle\", \"wb\") as h:\n","    pickle.dump(faiss.serialize_index(index), h)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["/content/vector_engine\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AtSC6oDDjtMA","executionInfo":{"status":"aborted","timestamp":1623834725032,"user_tz":-540,"elapsed":6,"user":{"displayName":"加納邦彦","photoUrl":"","userId":"06233728671996802409"}}},"source":[""],"execution_count":null,"outputs":[]}]}